{"meta":{"title":"JOKING","subtitle":"会当凌绝顶，一览众山小","description":"会当凌绝顶，一览众山小","author":"JOKING","url":"https://jokinglove.com/blog","root":"/blog/"},"pages":[{"title":"404 Not Found","date":"2020-10-17T08:07:03.861Z","updated":"2020-10-17T08:07:03.861Z","comments":true,"path":"404.html","permalink":"https://jokinglove.com/blog/404.html","excerpt":"","text":"啊奥，找不到了哎！！！"},{"title":"关于","date":"2019-07-02T16:00:00.000Z","updated":"2020-10-17T08:07:03.870Z","comments":true,"path":"about/index.html","permalink":"https://jokinglove.com/blog/about/index.html","excerpt":"","text":"关于我 码农，北京，hello worlding。。。 #### 关于主题 欢迎留言～"},{"title":"关于","date":"2019-07-02T16:00:00.000Z","updated":"2020-10-17T08:07:03.872Z","comments":true,"path":"books/index.html","permalink":"https://jokinglove.com/blog/books/index.html","excerpt":"","text":""},{"title":"gallery","date":"2020-04-02T01:08:21.000Z","updated":"2020-10-17T08:07:03.873Z","comments":true,"path":"gallery/index.html","permalink":"https://jokinglove.com/blog/gallery/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-10-17T08:07:03.873Z","updated":"2020-10-17T08:07:03.873Z","comments":true,"path":"friends/index.html","permalink":"https://jokinglove.com/blog/friends/index.html","excerpt":"","text":"恩，需要添加友链的，可以留下博客地址。。。"},{"title":"","date":"2020-10-17T08:07:03.874Z","updated":"2020-10-17T08:07:03.874Z","comments":true,"path":"projects/index.html","permalink":"https://jokinglove.com/blog/projects/index.html","excerpt":"","text":"项目列表 期待吧，少年。。。。。。"},{"title":"","date":"2020-10-17T08:07:03.873Z","updated":"2020-10-17T08:07:03.873Z","comments":true,"path":"mylist/index.html","permalink":"https://jokinglove.com/blog/mylist/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2020-10-17T08:07:03.873Z","updated":"2020-10-17T08:07:03.873Z","comments":true,"path":"group/categories/index.html","permalink":"https://jokinglove.com/blog/group/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-10-17T08:07:03.873Z","updated":"2020-10-17T08:07:03.873Z","comments":true,"path":"group/tags/index.html","permalink":"https://jokinglove.com/blog/group/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MongoDB-Springboot入门","slug":"others/database/MongoDB快速入门","date":"2020-09-08T16:00:00.000Z","updated":"2020-10-17T08:07:03.865Z","comments":true,"path":"2020/09/09/others/database/MongoDB快速入门/","link":"","permalink":"https://jokinglove.com/blog/2020/09/09/others/database/MongoDB快速入门/","excerpt":"MongoDB-Springboot 入门 一、MongoDB简介 MongoDB是一个由C++编写的，基于分布式文件存储的开源数据库系统。 通过添加节点可以快速扩容，保证服务器性能。 存储结构由键值（key, value）对组成。类似于 json 对象。 123456&#123; name: \"jim\", age: 25, status: \"A\", groups: [\"news\", \"sports\"]&#125;","text":"MongoDB-Springboot 入门 一、MongoDB简介 MongoDB是一个由C++编写的，基于分布式文件存储的开源数据库系统。 通过添加节点可以快速扩容，保证服务器性能。 存储结构由键值（key, value）对组成。类似于 json 对象。 123456&#123; name: \"jim\", age: 25, status: \"A\", groups: [\"news\", \"sports\"]&#125; 二、基本概念 Mongo是一个NoSQL 的文档数据库，可以对比 SQL 中的一些概念来理解。 SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins lookup 表关联 primary key primary key 主键,MongoDB自动将_id字段设置为主键 三、基础操作-增删改查 1、新增 db.collection.insertOne() db.collection.insertMany() db.collection.insert() 2、修改 db.collection.updateOne() db.collection.updateMany() db.collection.replaceOne() db.collection.update() db.person.update({name: ‘张三’}, {$set: {age: 30}}, {multi: true}) db.person.update({name: ‘张三’}, {$set: {‘address.$[i].no’: 50}}, {arrayFilters: [{‘i.province’: ‘上海’}]}) update person set age = 30 where name = ‘张三’； 3、删除 db.collection.deleteOne() db.collection.deleteMany() db.collection.remove() 4、查询 find db.collection.find() aggregate db.collection.aggregate Springboot-mongo 1、引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;&lt;/dependency&gt; 2、配置mongo客户端 1234567spring.data.mongodb.uri=mongodb://MEDBUser:MEDBUser@172.172.172.172:20000,172.172.172.173:20000,172.172.172.174:20000/MedicalExaminationDB4# 或者以下spring.data.mongodb.host=mongoserver.example.comspring.data.mongodb.port=27017spring.data.mongodb.database=testspring.data.mongodb.username=userspring.data.mongodb.password=secret 3、配置连接池（两种方式都可以，第二种有优先级更高） 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.joking.springbootmongo.config;import com.mongodb.MongoClientSettings;import org.bson.codecs.configuration.CodecRegistry;import org.bson.codecs.pojo.PojoCodecProvider;import org.springframework.boot.autoconfigure.mongo.MongoClientSettingsBuilderCustomizer;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.mongodb.core.MongoClientSettingsFactoryBean;import java.util.concurrent.TimeUnit;import static org.bson.codecs.configuration.CodecRegistries.fromProviders;import static org.bson.codecs.configuration.CodecRegistries.fromRegistries;@Configurationpublic class MongoConfig &#123; @Bean public MongoClientSettingsFactoryBean mongoClientSettingsFactoryBean() &#123; MongoClientSettingsFactoryBean factoryBean = new MongoClientSettingsFactoryBean(); factoryBean.setPoolMaxConnectionIdleTimeMS(5000); factoryBean.setPoolMaxSize(100); factoryBean.setPoolMinSize(10); return factoryBean; &#125; @Bean public MongoClientSettingsBuilderCustomizer mongoClientSettingsBuilderCustomizer() &#123; MongoClientSettingsBuilderCustomizer builder = clientSettingsBuilder -&gt; &#123; clientSettingsBuilder.applyToConnectionPoolSettings(settings -&gt; &#123; settings.maxConnectionIdleTime(1000, TimeUnit.SECONDS); settings.maxSize(100); settings.minSize(3); &#125;); clientSettingsBuilder.codecRegistry(this.codecRegistry()); &#125;; return builder; &#125; public CodecRegistry codecRegistry() &#123; return fromRegistries(MongoClientSettings.getDefaultCodecRegistry(), fromProviders(PojoCodecProvider.builder().automatic(true).build())); &#125;&#125; 4、测试 entity 123456789101112131415161718192021222324252627282930package com.joking.springbootmongo.entity;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import lombok.ToString;import java.util.List;@Data@ToString@NoArgsConstructor@AllArgsConstructorpublic class Person &#123; private String name; private int age; private List&lt;Address&gt; address; @Data @ToString @AllArgsConstructor public static class Address &#123; private String province; private String city; private String street; private int no; &#125;&#125; repository 12345678910111213141516171819package com.joking.springbootmongo.repository;import com.joking.springbootmongo.entity.Person;import org.bson.types.ObjectId;import org.springframework.data.mongodb.repository.MongoRepository;import org.springframework.stereotype.Repository;import java.util.List;import java.util.Optional;@Repositorypublic interface PersonRepository extends MongoRepository&lt;Person, ObjectId&gt; &#123; Optional&lt;List&lt;Person&gt;&gt; findByName(String name); Optional&lt;List&lt;Person&gt;&gt; findByAddressProvince(String name); Optional&lt;List&lt;Person&gt;&gt; findByNameRegex(String name);&#125; JunitTest 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.joking.springbootmongo.repository;import com.joking.springbootmongo.entity.Person;import lombok.extern.slf4j.Slf4j;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.data.mongodb.core.MongoTemplate;import static java.util.Arrays.asList;@Slf4j@SpringBootTestpublic class PersonRepositoryTest &#123; @Autowired private PersonRepository personRepository; @Autowired private MongoTemplate mongoTemplate; @Test public void testSave() &#123; Person person = new Person(); person.setName(\"张三\"); person.setAge(10); person.setAddress( asList( new Person.Address(\"北京\", \"海淀\", \"建外\", 10) ) ); Person save = personRepository.save(person); log.info(save.toString()); &#125; @Test public void findByNameTest() &#123; personRepository.findByName(\"张三\") .ifPresent(i -&gt; &#123; i.forEach(j -&gt; log.info(j.toString())); &#125;); &#125; @Test public void findByAddressProvinceTest() &#123; personRepository.findByAddressProvince(\"北京\") .ifPresent(i -&gt; &#123; i.forEach(j -&gt; log.info(j.toString())); &#125;); &#125; @Test public void findByNameRegexTest() &#123; personRepository.findByNameRegex(\"三\") .ifPresent(i -&gt; &#123; i.forEach(j -&gt; log.info(j.toString())); &#125;); &#125;&#125;","categories":[{"name":"database","slug":"database","permalink":"https://jokinglove.com/blog/group/categories/database/"}],"tags":[{"name":"database","slug":"database","permalink":"https://jokinglove.com/blog/group/tags/database/"},{"name":"mongo","slug":"mongo","permalink":"https://jokinglove.com/blog/group/tags/mongo/"},{"name":"springboot","slug":"springboot","permalink":"https://jokinglove.com/blog/group/tags/springboot/"}]},{"title":"1.0 RocketMq-基本概念","slug":"others/rocketmq/1.0-20200902-RocketMq-基本概念","date":"2020-09-01T16:00:00.000Z","updated":"2020-10-17T08:07:03.868Z","comments":true,"path":"2020/09/02/others/rocketmq/1.0-20200902-RocketMq-基本概念/","link":"","permalink":"https://jokinglove.com/blog/2020/09/02/others/rocketmq/1.0-20200902-RocketMq-基本概念/","excerpt":"基本概念 1、消息模型（Message Model） RocketMQ主要由 Producer、Broker、Consumer 三部分组成，其中 Producer 负责生产消息，Consumer 负责消费消息，Broker 负责存储消息。Broker 在实际部署过程中对应一台服务器，每个 Broker 可以存储多个 Topic 消息，每个 Topic 的消息也可以分片存储于不同的 Broker 。Message Queue 用于存储消息的物理地址，每个 Topic 中的消息地址存储于多个 Message Queue 中。ConsumerGroup 由多个Consumer 实例构成。","text":"基本概念 1、消息模型（Message Model） RocketMQ主要由 Producer、Broker、Consumer 三部分组成，其中 Producer 负责生产消息，Consumer 负责消费消息，Broker 负责存储消息。Broker 在实际部署过程中对应一台服务器，每个 Broker 可以存储多个 Topic 消息，每个 Topic 的消息也可以分片存储于不同的 Broker 。Message Queue 用于存储消息的物理地址，每个 Topic 中的消息地址存储于多个 Message Queue 中。ConsumerGroup 由多个Consumer 实例构成。 2、消息生产者（Producer） 负责生产消息，一般由业务系统负责生产消息。一个消息生产者会把业务应用系统里产生的消息发送到broker服务器。RocketMQ 提供多种发送方式，同步发送、异步发送、顺序发送、单向发送。同步和异步方式均需要Broker返回确认信息，单向发送不需要。 3、消息消费者（Consumer） 负责消费消息，一般是后台系统负责异步消费。一个消息消费者会从Broker服务器拉取消息、并将其提供给应用程序。从用户应用的角度而言提供了两种消费形式：pull and push。 4、主题（Topic） 表示一类消息的集合，每个主题包含若干条消息，每条消息只能属于一个主题，是RocketMQ进行消息订阅的基本单位。 5、代理服务器（Broker Server） 消息中转角色，负责存储消息，转发消息。代理服务器在RocketMQ系统中负责接收从生产者发送来的消息并存储、同时为消费者的拉取请求作准备。代理服务器也存储消息相关的元数据，包括消费者组、消费者进度偏移和主题以及队列消息等。 6、名称服务（Name Server） 名称服务充当路由消息的提供者。生产者或消费者能够通过名称服务查找各主题相应的Broker IP列表。多个Namesrv实例组成 集群，但相互独立，没有信息交换。 7、拉取式消费（Pull Consumer） Consumer消费的一种 类型，应用通常主动调用Consumer的拉消息 方法从Broker服务器拉消息、主动权由应用 控制。一旦获取了批量消息，应用就会启动消费过程。 8、推动式消费（Push Consumer） Consumer消费的一种类型，该模式下Broker收到数据后会主动推送给消息消费端，该消费模式一般实时性较高。 9、生产者组（Producer Group） 同一类Producer 的集合，这类 Producer 发送同一类消息且发送逻辑一直。如果发送的是事务消息且原始生产者在发送之后崩溃，则Broker服务器会联系同一生产者组的其他 生产者实例以提交或回溯消息。 10、消费者组（Consumer Group） 同一类Consumer的集合，这类 Consumer 通常消费同一类消息且消费逻辑一直。消费者组使得在消息消费方面，实现负载均衡和容错的目标变得非常容易。要注意的是，消费者组的消费者实例必须订阅完全相同的Topic。RocketMQ 支持两种消息模式：集群消费（Clustering） 和广播消费（Broadcasting）。 11、集群消费（Custering） 集群消费模式下，相同 Consumer Group 的每个 Consumer 实例平均分摊消息。 12、广播消费（Broadcasting） 广播消费模式下，相同 Consumer Group 的每个 Consumer 实例都接收全量的消息。 13、普通顺序消息（Normal Ordered Message） 普通顺序消费模式下，消费者通过同一个消费队列收到的消息是有顺序的，不同的消息队列收到的消息则可能是无顺序的。 14、严格顺序消息（Strictly Ordered Message） 严格顺序消息模式下，消费者收到的所有消息均是有顺序的。 15、消息（Message） 消息系统所传输信息的物理载体，生产和消费数据的最小单位，每条消息必须属于一个主题。RocketMQ 中每个消息拥有唯一的Message ID，且可以携带具有业务标识额 Key。系统提供了通过Message ID 和 Key 查询消息的功能。 16、标签（Tag） 为消息设置的标志，用于同一主题下区分不同类型的消息。来自同一业务单元的消息，可以根据不同业务目的在同一主题下设置不同的标签。标签能够有效地保持代码的清晰度和连贯性，并优化RocketMQ提供的查询系统。消费者可以根据 Tag 实现对不同字主题的不同消费逻辑，实现更好的扩展性。","categories":[{"name":"Java","slug":"Java","permalink":"https://jokinglove.com/blog/group/categories/Java/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://jokinglove.com/blog/group/tags/并发/"}]},{"title":"CountDownLatch的使用场景","slug":"others/java/CountDownLatch的两种用法","date":"2020-08-01T16:00:00.000Z","updated":"2020-10-17T08:07:03.866Z","comments":true,"path":"2020/08/02/others/java/CountDownLatch的两种用法/","link":"","permalink":"https://jokinglove.com/blog/2020/08/02/others/java/CountDownLatch的两种用法/","excerpt":"CountDownLatch的使用场景 CountDownLatch 是java1.5中引入的一个多线程之间协同的工具类，和它类似的还有CyclicBarrier类，今天我们主要讨论一下CountDownLatch 的两种使用场景。","text":"CountDownLatch的使用场景 CountDownLatch 是java1.5中引入的一个多线程之间协同的工具类，和它类似的还有CyclicBarrier类，今天我们主要讨论一下CountDownLatch 的两种使用场景。 一、控制线程的启动和结束 当有多个线程协同完成一个任务的时候，我们可以用来控制多个线程的开始和结束。 1234567891011121314151617181920212223242526272829303132333435363738public class Test &#123; public static void main(String[] ...args) &#123; // 开始信号 CountDownLatch startSignal = new CountDownLatch(1); // 结束信号 CountDownLatch doneSignal = new CountDownLatch(10); // 初始化线程 for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Worker(startSignal, doneSignal)).start(); &#125; // 这个时候我们做一些其他的事儿，等做完之后，所有的线程同时开始子任务 doSomethingElse(); // countDown 方法执行之后所有的任务同时开始执行 startSignal.countDown(); // 等待所有的任务执行结束 doneSignal.await(); &#125; public void doSomethingElse() &#123; System.out.println(\"任务开始前的工作。。。\"); &#125;&#125;public class Worker implements Runnable&#123; private final CountDownLatch startSignal; private final CountDownLatch doneSignal; public Worker(CountDownLatch startSignal, CountDownLatch doneSignal) &#123; this.startSignal = startSignal; this.doneSignal = doneSignal; &#125; @Override public void run() &#123; startSignal.await(); System.out.println(Thread.currentThread().getName() + \"开始工作了。。。\"); doneSignal.countDown(); &#125;&#125; 二、等待线程池中的所有线程执行完毕 当我们将一个任务分解成多个子任务来执行时，可以通过CountDownLatch来等待所有的子任务执行完毕。 12345678910111213141516171819202122232425262728293031323334public class Test&#123; public static void main(String[] ...args) &#123; CountDownLatch doneSignal = new CountDownLatch(10); // 初始化线程池 ExecutorService executor = Executors.newFixedThreadPool(10); // 提交任务到线程池 for(int i = 0; i &lt; 10; i++) &#123; executor.execute(new WorkRunnable(doneSignal, i)); &#125; // 等待所有的任务执行完毕 doneSignal.await(); &#125;&#125;class WorkRunnable implements Runnable&#123; private final CountDownLatch doneSignal; private final int i; public WorkRunnable(CountDownLatch doneSignal, int i) &#123; this.doneSignal = doneSignal; this.i = i; &#125; @Override public void run() &#123; doWork(i); doneSignal.countDown(); &#125; public void doWork(int i) &#123; System.out.println(String.format(\"%s - 正在工作。。。\", i)); &#125;&#125;","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"java","slug":"java","permalink":"https://jokinglove.com/blog/group/tags/java/"},{"name":"多线程","slug":"多线程","permalink":"https://jokinglove.com/blog/group/tags/多线程/"}]},{"title":"Neo4j CQL","slug":"others/database/Neo4j CQL","date":"2020-07-11T16:00:00.000Z","updated":"2020-10-17T08:07:03.865Z","comments":true,"path":"2020/07/12/others/database/Neo4j CQL/","link":"","permalink":"https://jokinglove.com/blog/2020/07/12/others/database/Neo4j CQL/","excerpt":"Neo4j CQL 一.Neo4j CQL命令 S.No. CQL命令/条 用法 1。 CREATE 创建 创建节点，关系和属性 2。 MATCH 匹配 检索有关节点，关系和属性数据 3。 RETURN 返回 返回查询结果 4。 WHERE 哪里 提供条件过滤检索数据 5。 DELETE 删除 删除节点和关系 6。 REMOVE 移除 删除节点和关系的属性 7。 ORDER BY以…排序 排序检索数据 8。 SET 组 添加或更新标签","text":"Neo4j CQL 一.Neo4j CQL命令 S.No. CQL命令/条 用法 1。 CREATE 创建 创建节点，关系和属性 2。 MATCH 匹配 检索有关节点，关系和属性数据 3。 RETURN 返回 返回查询结果 4。 WHERE 哪里 提供条件过滤检索数据 5。 DELETE 删除 删除节点和关系 6。 REMOVE 移除 删除节点和关系的属性 7。 ORDER BY以…排序 排序检索数据 8。 SET 组 添加或更新标签 1. CREATE 命令 作用： 创建没有属性的节点 使用属性创建节点 在没有属性的节点之间创建关系 使用属性创建节点之间的关系 为节点或者关系创建单个或多个标签 语法 1CREATE (&lt;node-name&gt;:&lt;label-name&gt;) 语法元素 描述 CREATE 它是一个Neo4j CQL命令。 它是我们要创建的节点名称。 它是一个节点标签名称 举例 create (emp:Employee) // 创建一个标签为Employee 的emp节点 create (dept:Dept {deptno: 10, dname: “啊哈”, location: “北京市海淀区”}) 2. MATCH 命令 作用： 从数据库获取有关节点和属性的数据 从数据库获取有关节点，关系和属性的数据 语法： 1MATCH ( &lt;node-name&gt;:&lt;label-name&gt; ) 语法元素 描述 这是我们要创建一个节点名称。 这是一个节点的标签名称 注意事项： Neo4j数据库服务器使用此将此节点详细信息存储在Database.As中作为Neo4j DBA或Developer，我们不能使用它来访问节点详细信息。 Neo4j数据库服务器创建一个作为内部节点名称的别名。作为Neo4j DBA或Developer，我们应该使用此标签名称来访问节点详细信息。 3. RETURN命令 作用 检索节点的某些属性 检索节点的所有属性 检索节点和关联关系的某些属性 检索节点和关联关系的所有属性 语法 1234RETURN &lt;node-name&gt;.&lt;property1-name&gt;, ........ &lt;node-name&gt;.&lt;propertyn-name&gt; 语法元素 描述 它是我们将要创建的节点名称。 … 属性是键值对。 定义要分配给创建节点的属性的名称 注意事项： RETURN 和 MATCH 一样不能单独使用，需要两个一起配合使用。 4. MATCH + RETURN 语法： 12MATCH CommandRETURN Command 示例 12MATCH (dept: Dept)RETURN dept.deptno,dept.dname 1MATCH (dept: Dept) return dept // 可以查看 graph 5、CREATE + MATCH + RETURN命令 本示例演示如何使用属性和这两个节点之间的关系创建两个节点。 二. Neo4j 函数 S.No. 定制列表功能 用法 1。 String 字符串 它们用于使用String字面量。 2。 Aggregation 聚合 它们用于对CQL查询结果执行一些聚合操作。 3。 Relationship 关系 他们用于获取关系的细节，如startnode，endnode等。 三. Neo4j CQL 数据类型 这些数据类型与Java语言类似。他们用于定义节点或者关系的属性 Neo4j CQL 支持以下数据类型： S.No. CQL数据类型 用法 1. boolean 用于表示布尔文字：true，false。 2. byte 用于表示8位整数。 3. short 用于表示16位整数。 4. int 用于表示32位整数。 5. long 用于表示64位整数。 6. float I用于表示32位浮点数。 7. double 用于表示64位浮点数。 8. char 用于表示16位字符。 9. String 用于表示字符串。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"Database","slug":"Database","permalink":"https://jokinglove.com/blog/group/tags/Database/"}]},{"title":"Centos7 网络配置","slug":"others/linux/20200102-centos7网络设置","date":"2020-01-01T16:00:00.000Z","updated":"2020-10-17T08:07:03.867Z","comments":true,"path":"2020/01/02/others/linux/20200102-centos7网络设置/","link":"","permalink":"https://jokinglove.com/blog/2020/01/02/others/linux/20200102-centos7网络设置/","excerpt":"刚安装好的 Centos7 发现没网，用命令 ip addr 查看返回如下：","text":"刚安装好的 Centos7 发现没网，用命令 ip addr 查看返回如下： 我们发现，只有 lo，ens33 的网卡中没有 IP，一般情况，就电脑启动的时候，没有激活网卡 ens33，通过以下几步我们可以解决这个问题： 1、打开 /etc/sysconfig/network-scripts/ifcfg-ens33文件： 2、图中 NOBOOT=no, 代表开启不自动启动网卡 ens33，我们修改为 NOBOOT=yes, BOOTPROTO=dhcp 代表自动获取 ip。 3、重启加载网络配置文件 1nmcli connection reload 4、重新执行 ip addr 查看结果如下，网络正常。 5、显示所有网络设备详情信息 1nmcli device show","categories":[{"name":"linux","slug":"linux","permalink":"https://jokinglove.com/blog/group/categories/linux/"}],"tags":[{"name":"tools","slug":"tools","permalink":"https://jokinglove.com/blog/group/tags/tools/"}]},{"title":"Zookeeper 简介及核心概念","slug":"others/microservice/20191231-zookeeper","date":"2019-12-30T16:00:00.000Z","updated":"2020-10-17T08:07:03.868Z","comments":true,"path":"2019/12/31/others/microservice/20191231-zookeeper/","link":"","permalink":"https://jokinglove.com/blog/2019/12/31/others/microservice/20191231-zookeeper/","excerpt":"一、Zookeeper简介 Zookeeper 是一个开源的分布式协调服务，目前由 Apache 进行维护，官网是： http://zookeeper.apache.org zookeeper 可以用于实现分布式系统中常见的发布/订阅、负载均衡、命令服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。它具有以下特性： 顺序一致性： 从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到 Zookeeper 中； 原子性： 所有事务请求的处理结果在整个集群中所有机器上都是一致的；不存在部分机器应用了该事物，而另一部分没有应用到的情况； 单一视图: 所有客户端看到的服务端数据模型都是一致的； 可靠性: 一旦一个事务被成功应用后，Zookeeper 可以保证客户端立即可以 读取到这个事务变更后的最新状态的数据。","text":"一、Zookeeper简介 Zookeeper 是一个开源的分布式协调服务，目前由 Apache 进行维护，官网是： http://zookeeper.apache.org zookeeper 可以用于实现分布式系统中常见的发布/订阅、负载均衡、命令服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。它具有以下特性： 顺序一致性： 从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到 Zookeeper 中； 原子性： 所有事务请求的处理结果在整个集群中所有机器上都是一致的；不存在部分机器应用了该事物，而另一部分没有应用到的情况； 单一视图: 所有客户端看到的服务端数据模型都是一致的； 可靠性: 一旦一个事务被成功应用后，Zookeeper 可以保证客户端立即可以 读取到这个事务变更后的最新状态的数据。 二、Zookeeper 设计目标 Zookeeper 致力于为那些高吞吐的大型分布式系统提供一个高性能、高可用、且具有严格顺序访问控制能力的分布式协调服务。它具有以下四个目标： 2.1 目标一：简单的数据模型 Zookeeper 通过属性结构来存储数据，它由一系列被称为 ZNode 的数据节点组成，类似于常见的文件系统。不过和常见的文件系统不同，Zookeeper 将数据全量存储在内存中，一次来实现高吞吐，减少访问延迟。 2.2 目标二：构建集群 可以有一组 Zookeeper 服务构成 Zookeeper 集群，集群中每台机器都会单独在内存中维护自身的状态，并且每台机器之间都保持着通讯，只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务。 2.3 目标三：顺序访问 对于来自客户端的每个更新请求， Zookeeper 都会分配一个全局唯一的递增 ID，这个 ID 反映了所有事务请求的先后顺序。 2.4 目标四：高性能高可用 Zookeeper 将数据全量存储在内存中以保持高性能，并通过服务集群来实现高可用。由于 Zookeeper 的所有更新和删除都是基于事务的，所以其在读多写少的应用场景中有着很高的性能表现。 三、核心概念 3.1 集群角色 Zookeeper 集群中的机器分为以下三种角色： Leader : 为客户端提供读写服务，并维护集群状态，它是由集群选举所产生的； Follower : 为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态。同时也参与写操作 “过半写成功” 的次略和 Leader 的选举； Observer ： 为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态，但不参与写操作“过半写成功”的策略和 Leader 的选举，因此 Observer 可以在不影响写性能的情况下提升集群的读性能。 3.2 回话 Zookeeper 客户端通过 TCP 长连接连接到服务集群，回话（Session） 从第一次连接开始就已经建立，之后通过心跳检测机制来保持有效的会话状态。通过这个连接，客户端可以发送请求并接受响应，同时也可以接收到 Watch 事件的通知。 关于会话中另外一个核心的概念是 sessionTimeOut(会话超时时间)，当由于网络故障或者客户端主动断开等原因，导致连接断开，此时只要在会话超时时间之内重新建立连接，则之前创建的会话依然有效。 3.3 数据节点 Zookeeper 数据模型是由一系列基本数据单元 Znode(数据节点) 组成的节点树，其中根节点为 /。每个节点上都会保存自己的数据和节点信息。Zookeeper 中节点可以分为两大类： 持久节点 ：节点一旦创建，除非被主动删除，否则一直存在； 临时节点 ：一旦创建该节点的客户端会话失效，则所有该客户端创建的临时节点都会被删除。 临时节点和持久节点都可以添加一个特殊的属性：SEQUENTIAL，代表该节点是否具有递增属性。如果指定该属性，那么在这个节点创建时，Zookeeper 会自动在其节点名称后面追加一个由父节点维护的递增数字。 3.4 节点信息 每个 ZNode 节点在存储数据的同时，都会维护一个叫做 Stat 的数据结构，里面存储了关于该节点的全部状态信息。如下： 状态属性 说明 czxid 数据节点创建时的事务 ID ctime 数据节点创建时的时间 mzxid 数据节点最后一次更新时的事务 ID mtime 数据节点最后一次更新时的时间 pzxid 数据节点的子节点最后一次被修改时的事务 ID cversion 子节点的更改次数 version 节点数据的更改次数 aversion 节点的 ACL 的更改次数 ephemeralOwner 如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0 dataLength 数据内容的长度 numChildren 数据节点当前的子节点个数 3.5 Watcher Zookeeper 中一个常用的功能是 Watcher(事件监听器)，它允许用户在指定节点上针对感兴趣的事件注册监听，当事件发生时，监听器会被触发，并将事件信息推送到客户端。该机制是 Zookeeper 实现分布式协调服务的重要特性。 3.6 ACL Zookeeper 采用 ACL(Access Control Lists) 策略来进行权限控制，类似于 UNIX 文件系统的权限控制。它定义了如下五种权限： CREATE：允许创建子节点； READ：允许从节点获取数据并列出其子节点； WRITE：允许为节点设置数据； DELETE：允许删除子节点； ADMIN：允许为节点设置权限。 四、ZAB协议 4.1 ZAB协议与数据一致性 ZAB 协议是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。通过该协议，Zookeepe 基于主从模式的系统架构来保持集群中各个副本之间数据的一致性。具体如下： Zookeeper 使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用原子广播协议将数据状态的变更以事务 Proposal 的形式广播到所有的副本进程上去。如下图： 具体流程如下： 所有的事务请求必须由唯一的 Leader 服务来处理，Leader 服务将事务请求转换为事务 Proposal，并将该 Proposal 分发给集群中所有的 Follower 服务。如果有半数的 Follower 服务进行了正确的反馈，那么 Leader 就会再次向所有的 Follower 发出 Commit 消息，要求将前一个 Proposal 进行提交。 4.2 ZAB协议的内容 ZAB 协议包括两种基本的模式，分别是崩溃恢复和消息广播： 1.崩溃恢复 当整个服务框架在启动过程中，或者当 Leader 服务器出现异常时，ZAB 协议就会进入恢复模式，通过过半选举机制产生新的 Leader，之后其他机器将从新的 Leader 上同步状态，当有过半机器完成状态同步后，就退出恢复模式，进入消息广播模式。 2.消息广播 ZAB 协议的消息广播过程使用的是原子广播协议。在整个消息的广播过程中，Leader 服务器会每个事物请求生成对应的 Proposal，并为其分配一个全局唯一的递增的事务 ID(ZXID)，之后再对其进行广播。具体过程如下： Leader 服务会为每一个 Follower 服务器分配一个单独的队列，然后将事务 Proposal 依次放入队列中，并根据 FIFO(先进先出) 的策略进行消息发送。Follower 服务在接收到 Proposal 后，会将其以事务日志的形式写入本地磁盘中，并在写入成功后反馈给 Leader 一个 Ack 响应。当 Leader 接收到超过半数 Follower 的 Ack 响应后，就会广播一个 Commit 消息给所有的 Follower 以通知其进行事务提交，之后 Leader 自身也会完成对事务的提交。而每一个 Follower 则在接收到 Commit 消息后，完成事务的提交。 五、Zookeeper 的典型应用场景 5.1 数据的发布/订阅 数据的发布/订阅系统，通常也用作配置中心。在分布式系统中，你可能有成千上万个服务节点，如果想要对所有服务的某项配置进行更改，由于数据节点过多，你不可逐台进行修改，而应该在设计时采用统一的配置中心。之后发布者只需要将新的配置发送到配置中心，所有服务节点即可自动下载并进行更新，从而实现配置的集中管理和动态更新。 Zookeeper 通过 Watcher 机制可以实现数据的发布和订阅。分布式系统的所有的服务节点可以对某个 ZNode 注册监听，之后只需要将新的配置写入该 ZNode，所有服务节点都会收到该事件。 5.2 命名服务 在分布式系统中，通常需要一个全局唯一的名字，如生成全局唯一的订单号等，Zookeeper 可以通过顺序节点的特性来生成全局唯一 ID，从而可以对分布式系统提供命名服务。 5.3 Master 选举 分布式系统一个重要的模式就是主从模式 (Master/Salves)，Zookeeper 可以用于该模式下的 Matser 选举。可以让所有服务节点去竞争性地创建同一个 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，这样该服务节点就可以成为 Master 节点。 5.4 分布式锁 可以通过 Zookeeper 的临时节点和 Watcher 机制来实现分布式锁，这里以排它锁为例进行说明： 分布式系统的所有服务节点可以竞争性地去创建同一个临时 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，此时可以认为该节点获得了锁。其他没有获得锁的服务节点通过在该 ZNode 上注册监听，从而当锁释放时再去竞争获得锁。锁的释放情况有以下两种： 当正常执行完业务逻辑后，客户端主动将临时 ZNode 删除，此时锁被释放； 当获得锁的客户端发生宕机时，临时 ZNode 会被自动删除，此时认为锁已经释放。 当锁被释放后，其他服务节点则再次去竞争性地进行创建，但每次都只有一个服务节点能够获取到锁，这就是排他锁。 5.5 集群管理 Zookeeper 还能解决大多数分布式系统中的问题： 如可以通过创建临时节点来建立心跳检测机制。如果分布式系统的某个服务节点宕机了，则其持有的会话会超时，此时该临时节点会被删除，相应的监听事件就会被触发。 分布式系统的每个服务节点还可以将自己的节点状态写入临时节点，从而完成状态报告或节点工作进度汇报。 通过数据的订阅和发布功能，Zookeeper 还能对分布式系统进行模块的解耦和任务的调度。 通过监听机制，还能对分布式系统的服务节点进行动态上下线，从而实现服务的动态扩容。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://jokinglove.com/blog/group/tags/大数据/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://jokinglove.com/blog/group/tags/zookeeper/"}]},{"title":"Java 中的 SPI 机制","slug":"others/java/20190913-java-spi","date":"2019-09-12T16:00:00.000Z","updated":"2020-10-17T08:07:03.865Z","comments":true,"path":"2019/09/13/others/java/20190913-java-spi/","link":"","permalink":"https://jokinglove.com/blog/2019/09/13/others/java/20190913-java-spi/","excerpt":"1、 SPI 简介 SPI 全称 Service Provider Interface，是一种服务发现机制。SPI 的本质是将接口类的全限定名配置在文件中，并有服务加载器读取配置文件，加载实现类，这样可以在运行时，动态为接口替换实现类。正因为此特性，在开发中很容易通过 SPI 机制为我们的应用程序提供扩展功能。好多框架中都用到了 SPI 机制来扩展自身的功能。比如：Dubbo，Springboot 等，都是用 SPI 机制来加载实现的。","text":"1、 SPI 简介 SPI 全称 Service Provider Interface，是一种服务发现机制。SPI 的本质是将接口类的全限定名配置在文件中，并有服务加载器读取配置文件，加载实现类，这样可以在运行时，动态为接口替换实现类。正因为此特性，在开发中很容易通过 SPI 机制为我们的应用程序提供扩展功能。好多框架中都用到了 SPI 机制来扩展自身的功能。比如：Dubbo，Springboot 等，都是用 SPI 机制来加载实现的。 2、SPI 示例 2.1 Java SPI 实例 前面介绍了 SPI 机制的原理，本节通过一个简单的示例演示 Java SPI 的使用方法。 首先，我们定义一个接口，名称为 Animal。 12345package com.joking.spi.jdk;public interface Animal &#123; void say();&#125; 接下来定义两个实现类，分别为 Cat 和 Dog。 1234567891011121314151617package com.joking.spi.jdk;public class Cat implements Animal &#123; @Override public void say() &#123; System.out.println(\"阿猫🐈！\"); &#125;&#125;package com.joking.spi.jdk;public class Dog implements Animal &#123; @Override public void say() &#123; System.out.println(\"阿狗🐶！\"); &#125;&#125; 然后我们在 META-INF/services 文件夹下创建一个文件，名称为 Animal 的全限定名 com.joking.spi.jdk.Animal 。文件内容为实现类的全限定的类名，如下： 12com.joking.spi.jdk.Catcom.joking.spi.jdk.Dog 最后，我们写个main 来测试。 1234567891011121314package com.joking;import com.joking.spi.jdk.Animal;import java.util.ServiceLoader;public class Main &#123; public static void main(String[] args) &#123; ServiceLoader&lt;Animal&gt; animals = ServiceLoader.load(Animal.class); animals.forEach(System.out::println); animals.forEach(Animal::say); &#125;&#125; 我们可以看到测试结果如下： 最后整个项目看起来就是这样的","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"java","slug":"java","permalink":"https://jokinglove.com/blog/group/tags/java/"},{"name":"SPI","slug":"SPI","permalink":"https://jokinglove.com/blog/group/tags/SPI/"}]},{"title":"正则表达式","slug":"others/20190807-正则表达式","date":"2019-08-06T16:00:00.000Z","updated":"2020-10-17T08:07:03.863Z","comments":true,"path":"2019/08/07/others/20190807-正则表达式/","link":"","permalink":"https://jokinglove.com/blog/2019/08/07/others/20190807-正则表达式/","excerpt":"","text":"1、基本匹配 正则表达式就是在执行搜索时的格式，它由一些字母和数字组合而成，例如：一个正则表达式 the，它表示一个规则：由字母 t 开始，接着是 h，在接着是 e。 “the” =&gt; The fat cat sat on the mat 2、元字符 正则表达式主要依赖于元字符，元字符不代表他们本身的字面意思，他们都有特殊的含义。一些元字符写在方括号的时候有一些特殊的意思，以下是一些元字符的介绍： 元字符 描述 . 句号匹配任意单个字符除了换行符。 [] 字符种类。匹配方括号内的任意字符。 [^ ] 否定的字符种类，匹配除了方括号里的任意字符。 * 匹配 &gt;= 0 个重复的在 * 号之前的字符。 + 匹配 &gt;= 1 个重复的 + 号前的字符。 ? 标记 ？之前的字符为可选。 {n,m} 匹配 num 个大括号之前的字符（n &lt;= num &lt;= m). (xyz) 字符集，匹配与xyz完全相等的字符串。 | 或运算符，匹配符号前或后的字符。 \\ 转义字符，用于匹配一些保留的字符[] () {} . * + ? ^ $ \\ | ^ 从开始行开始匹配。 $ 从末端开始匹配。 2.1 点运算符 . . 是元字符中最简单的例子，. 匹配任意单个字符，但不匹配换行符，例如，表达式 .ar 匹配一个任意字符后面跟着 a 和 r 的字符串。 “.ar” =&gt; car par ked in the gar age. 2.2 字符集 字符集也叫做字符类。方括号用来指定一个字符集。在方括号中使用连字符来指定字符集的范围，在方括号中的字符集不关心书序，例如，表达式 [Tt]he 匹配 the 和 The. “[Tt]he” =&gt;The car parked in the garage. 方括号的句号就表示句号。表达式 ar[.] 匹配 ar. 字符串。 “ar[.]” =&gt; A garage is a good place to park a car. 2.2.1 否定字符集 一般来说 ^ 表示一个字符串的开头，但它用在一个方括号的开头的时候，它表示这个字符集是否定的。例如，表达式 [^c]ar 匹配一个后面跟着 ar 的除了 c 的任意字符。 “[^c]ar” =&gt; The carparked in thegarage. 2.3 重复次数 后面跟着元字符 +, * or ? 的，用来指定匹配子模式的次数。这些元字符在不同的情况下有着不同的意思。 2.3.1 * 号 * 号匹配 在 * 之前出现的大于等于0次，例如 a* 匹配以 0 或者更多个 a 开头的字符，因为有 0 个这个条件，其实就是匹配了所有的字符，表达式 [a-z]* 匹配一个行中所有小写字母开头的字符。 “[a-z]*” =&gt; The car parked in the garage #21. * 字符和 . 字符搭配可以匹配所有的字符 .*, *和表示匹配空格的字符 \\s连起来用，如表达式 \\s*cat\\s* 匹配 0 或更多个空格开头 和 0 或更多个空格结尾的 cat 字符串。 “\\s*cat\\s*” =&gt; The fatcat sat on the concatenation. 2.3.2 +号 +号匹配+号之前的字符出现 &gt;=1 次. 例如表达式c.+t 匹配以首字母c开头以t结尾,中间跟着任意个字符的字符串 “c.+t” =&gt; The fatcat sat on the mat. 2.3.3 ?号 在正则表达式中元字符 ? 标记在符号前面的字符为可选, 即出现 0 或 1 次. 例如, 表达式 [T]?he 匹配字符串 he 和 The. “[T]he” =&gt;The car is parked in the garage. “[T]?he” =&gt;The car is parked in the garage. 2.4 {}号 在正则表达式中 {} 是一个量词, 常用来一个或一组字符可以重复出现的次数. 例如, 表达式 [0-9]{2,3}匹配最少 2 位最多 3 位 0~9 的数字. “[0-9]{2,3}” =&gt; The number was 9.9997 but we rounded it off to10.0. 我们可以省略第二个参数. 例如, [0-9]{2,} 匹配至少两位 0~9 的数字. “[0-9]{2,}” =&gt; The number was 9.9997 but we rounded it off to10.0. 如果逗号也省略掉则表示重复固定的次数. 例如, [0-9]{3} 匹配3位数字 “[0-9]{3}” =&gt; The number was 9.9997 but we rounded it off to 10.0. 2.5 (…) 特征标群 特征标群是一组写在 (...) 中的子模式. 例如之前说的 {} 是用来表示前面一个字符出现指定次数. 但如果在 {} 前加入特征标群则表示整个标群内的字符重复 N 次. 例如, 表达式 (ab)* 匹配连续出现 0 或更多个 ab. 我们还可以在 () 中用或字符 | 表示或. 例如, (c|g|p)ar 匹配 car 或 gar 或 par. “(c|g|p)ar” =&gt; Thecar isparked in thegarage. 2.6 | 或运算符 或运算符就表示或, 用作判断条件. 例如 (T|t)he|car 匹配 (T|t)he 或 car. “(T|t)he|car” =&gt;The car is parked in the garage. 2.7 转码特殊字符 反斜线 \\ 在表达式中用于转码紧跟其后的字符. 用于指定 { } [ ] / \\ + * . $ ^ | ? 这些特殊字符. 如果想要匹配这些特殊字符则要在其前面加上反斜线 \\. 例如 . 是用来匹配除换行符外的所有字符的. 如果想要匹配句子中的 . 则要写成 \\. 以下这个例子 \\.?是选择性匹配. “(f|c|m)at\\.?” =&gt; Thefat cat sat on themat. 2.8 锚点 在正则表达式中, 想要匹配指定开头或结尾的字符串就要使用到锚点. ^ 指定开头, $ 指定结尾. 2.8.1 ^ 号 ^ 用来检查匹配的字符串是否在所匹配字符串的开头. 例如, 在 abc 中使用表达式 ^a 会得到结果 a. 但如果使用 ^b 将匹配不到任何结果. 因为在字符串 abc中并不是以 b 开头. 例如, ^(T|t)he 匹配以 The 或 the 开头的字符串. “(T|t)he” =&gt;The car is parked in the garage. “^(T|t)he” =&gt;The car is parked in the garage. 2.8.2 $号 同理于 ^ 号, $ 号用来匹配字符是否是最后一个. 例如, (at\\.)$ 匹配以 at. 结尾的字符串. “(at.)” =&gt; The fat cat. sat. on the mat. “(at.)$” =&gt; The fat cat. sat. on the mat. 3.简写字符集 正则表达式提供一些常用的字符集简写. 如下: 简写 描述 . 除换行符外的所有字符 \\w 匹配所有字母数字, 等同于 [a-zA-Z0-9_] \\W 匹配所有非字母数字, 即符号, 等同于: [^\\w] \\d 匹配数字: [0-9] \\D 匹配非数字: [^\\d] \\s 匹配所有空格字符, 等同于: [\\t\\n\\f\\r\\p{Z}] \\S 匹配所有非空格字符: [^\\s] \\f 匹配一个换页符 \\n 匹配一个换行符 \\r 匹配一个回车符 \\t 匹配一个制表符 \\v 匹配一个垂直制表符 \\p 匹配 CR/LF (等同于 \\r\\n)，用来匹配 DOS 行终止符 4.零宽度断言（前后预查） 先行断言和后发断言都属于非捕获簇(不捕获文本 ，也不针对组合计进行计数). 先行断言用于判断所匹配的格式是否在另一个确定的格式之前, 匹配结果不包含该确定格式(仅作为约束). 例如, 我们想要获得所有跟在 $ 符号后的数字, 我们可以使用正后发断言 (?&lt;=\\$)[0-9\\.]*. 这个表达式匹配 $ 开头, 之后跟着 0,1,2,3,4,5,6,7,8,9,. 这些字符可以出现大于等于 0 次. 零宽度断言如下: 符号 描述 ?= 正先行断言-存在 ?! 负先行断言-排除 ?&lt;= 正后发断言-存在 ?&lt;! 负后发断言-排除 4.1 ?=… 正先行断言 ?=... 正先行断言, 表示第一部分表达式之后必须跟着 ?=...定义的表达式. 返回结果只包含满足匹配条件的第一部分表达式. 定义一个正先行断言要使用 (). 在括号内部使用一个问号和等号: (?=...). 正先行断言的内容写在括号中的等号后面. 例如, 表达式 (T|t)he(?=\\sfat) 匹配 The 和 the, 在括号中我们又定义了正先行断言 (?=\\sfat) ,即 The 和 the 后面紧跟着 (空格)fat. “(T|t)he(?=\\sfat)” =&gt;The fat cat sat on the mat. 4.2 ?!... 负先行断言 负先行断言 ?! 用于筛选所有匹配结果, 筛选条件为 其后不跟随着断言中定义的格式. 正先行断言 定义和 负先行断言 一样, 区别就是 = 替换成 ! 也就是 (?!...). 表达式 (T|t)he(?!\\sfat) 匹配 The 和 the, 且其后不跟着 (空格)fat. “(T|t)he(?!\\sfat)” =&gt; The fat cat sat on the mat. 4.3 ?&lt;==... 正后发断言 正后发断言 记作(?&lt;=...) 用于筛选所有匹配结果, 筛选条件为 其前跟随着断言中定义的格式. 例如, 表达式 (?&lt;=(T|t)he\\s)(fat|mat) 匹配 fat 和 mat, 且其前跟着 The 或 the. “(?&lt;=(T|t)he\\s)(fat|mat)” =&gt; The fat cat sat on the mat. 4.4 ?&lt;!…负后发断言 负后发断言 记作 (?&lt;!...) 用于筛选所有匹配结果, 筛选条件为 其前不跟随着断言中定义的格式. 例如, 表达式 (?&lt;!(T|t)he\\s)(cat) 匹配 cat, 且其前不跟着 The 或 the. “(?&lt;!(T|t)he\\s)(cat)” =&gt; The cat sat on cat. 5. 标志 标志也叫模式修正符, 因为它可以用来修改表达式的搜索结果. 这些标志可以任意的组合使用, 它也是整个正则表达式的一部分. 标志 描述 i 忽略大小写. g 全局搜索. m 多行的: 锚点元字符 ^ $ 工作范围在每行的起始. 5.1 忽略大小写（Case Insensitive） 修饰语 i 用于忽略大小写. 例如, 表达式 /The/gi 表示在全局搜索 The, 在后面的 i 将其条件修改为忽略大小写, 则变成搜索 the 和 The, g 表示全局搜索. “The” =&gt;The_ fat cat sat on the mat. “/The/gi” =&gt;The fat cat sat onthe mat. 5.2 全局搜索 （Global search） 修饰符 g 常用于执行一个全局搜索匹配, 即(不仅仅返回第一个匹配的, 而是返回全部). 例如, 表达式 /.(at)/g 表示搜索 任意字符(除了换行) + at, 并返回全部结果. “/.(at)/” =&gt; The fat cat sat on the mat. “/.(at)/g” =&gt; The fat cat sat on the mat. 5.3 多行修饰符 （Multiline) 多行修饰符 m 常用于执行一个多行匹配. 像之前介绍的 (^,$) 用于检查格式是否是在待检测字符串的开头或结尾. 但我们如果想要它在每行的开头和结尾生效, 我们需要用到多行修饰符 m. 例如, 表达式 /at(.)?$/gm 表示小写字符 a 后跟小写字符 t , 末尾可选除换行符外任意字符. 根据 m 修饰符, 现在表达式匹配每行的结尾. “/.at(.)?$/” =&gt; The fat cat sat on the mat. “/.at(.)?$/gm” =&gt; The fat cat sat on the mat. 6. 贪婪匹配与惰性匹配 （Greedy vs lazy matching） 正则表达式默认采用贪婪匹配模式，在该模式下意味着会匹配尽可能长的子串。我们可以使用 ? 将贪婪匹配模式转化为惰性匹配模式。 “/(.*at)/” =&gt; The fat cat sat on the mat. “/(.*?at)/” =&gt; The fat cat sat on the mat.","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"}]},{"title":"Web Service 相关概念和总结","slug":"others/20190617-webservice","date":"2019-07-09T16:00:00.000Z","updated":"2020-10-17T08:07:03.863Z","comments":true,"path":"2019/07/10/others/20190617-webservice/","link":"","permalink":"https://jokinglove.com/blog/2019/07/10/others/20190617-webservice/","excerpt":"Web Service 相关概念和总结 一. 序言 序言什么的都不重要…… 二、Web Service 是什么 Web Service 是一种跨编程语言和跨操作系统平台的远程调用技术 所谓跨编程语言和跨操作平台，就是说服务端程序采用 java 编写，客户端程序则可以采用其他的编程语言编写，反之亦然！跨操作系统平台是指服务端程序和客户端程序可以在不同的操作系统上运行。 远程调用，是指一台计算机 a 上的一个程序可以调用到另外一台计算机 b 上的一个对象的方法。","text":"Web Service 相关概念和总结 一. 序言 序言什么的都不重要…… 二、Web Service 是什么 Web Service 是一种跨编程语言和跨操作系统平台的远程调用技术 所谓跨编程语言和跨操作平台，就是说服务端程序采用 java 编写，客户端程序则可以采用其他的编程语言编写，反之亦然！跨操作系统平台是指服务端程序和客户端程序可以在不同的操作系统上运行。 远程调用，是指一台计算机 a 上的一个程序可以调用到另外一台计算机 b 上的一个对象的方法。 其实可以从多个角度来理解 WebService，从表面上看，WebService就是一个应用程序向外界暴露出一个能通过Web进行调用的API，也就是说能用编程的方法通过 Web来调用这个应用程序。我们把调用这个WebService的应用程序叫做客户端，而把提供这个WebService的应用程序叫做服务端。从深层次看，WebService是建立可互操作的分布式应用程序的新平台，是一个平台，是一套标准。它定义了应用程序如何在Web上实现互操作性，你可以用任何你喜欢的语言，在任何你喜欢的平台上写Web service ，只要我们可以通过Web service标准对这些服务进行查询和访问。 WebService平台需要一套协议来实现分布式应用程序的创建。任何平台都有它的数据表示方法和类型系统。要实现互操作性，WebService平台必须提供一套标准的类型系统，用于沟通不同平台、编程语言和组件模型中的不同类型系统。Web service平台必须提供一种标准来描述 Web service，让客户可以得到足够的信息来调用这个Web service。最后，我们还必须有一种方法来对这个Web service进行远程调用,这种方法实际是一种远程过程调用协议(RPC)。为了达到互操作性，这种RPC协议还必须与平台和编程语言无关。 三、WebService 平台技术 ​ XML + XSD，SOAP 和 WSDL 就是构成 WebService 平台的三大技术。 3.1、XML + XSD ​ WebService 采用 HTTP 协议传输数据，采用 XML 格式封装数据 (即 XML 中说明调用远程服务对象的哪个方法，传递的参数是什么，以及服务对象的返回结果是什么）。 ​ XML 是 WebService 平台中表示数据的格式。除了易于简历和易于分析外，XML 主要的有点在于它既是平台无关的，优势厂商无关的。无关性是比优越性更重要的：软件厂商是不会选择一个由竞争对手发明的技术。 ​ XML解决了数据表示的问题，但它没有定义一套标准的数据类型，更没有说怎么去扩展这套数据类型。例如，整形数到底代表什么？16位，32位，64位？这些细节对实现互操作性很重要。XML Schema(XSD)就是专门解决这个问题的一套标准。它定义了一套标准的数据类型，并给出了一种语言来扩展这套数据类型。WebService平台就 是用XSD来作为其数据类型系统的。当你用某种语言(如VB.NET或C#)来构造一个Web service时，为了符合WebService标准，所 有你使用的数据类型都必须被转换为XSD类型。你用的工具可能已经自动帮你完成了这个转换，但你很可能会根据你的需要修改一下转换过程。 3.2、SOAP ​ WebService 通过 HTTP 协议发送请求和接受结果时，发送的请求内容和结果内容都采用 XML 格式封装，并增加一些特定的 HTTP 消息头，以说明 HTTP 消息的内容格式，这些特定的 HTTP 消息头和 XML 内容格式就是 SOAP 协议。SOAP 协议提供了标准的 RPC 方法来调用 WebService。 SOAP协议 = HTTP协议 + XML 数据格式 ​ SOAP协议定义了SOAP消息的格式，SOAP协议是基于HTTP协议的，SOAP也是基于XML和XSD的，XML是SOAP的数据编码方式。打个比 喻：HTTP就是普通公路，XML就是中间的绿色隔离带和两边的防护栏，SOAP就是普通公路经过加隔离带和防护栏改造过的高速公路。 3.3、 WSDL 好比我们去商店买东西，首先要知道商店里有什么东西可买，然后再来购买，商家的做法就是张贴广告海报。 WebService也一样，WebService客户端要调用一个WebService服务，首先要有知道这个服务的地址在哪，以及这个服务里有什么方 法可以调用，所以，WebService务器端首先要通过一个WSDL文件来说明自己家里有啥服务可以对外调用，服务是什么（服务中有哪些方法，方法接受 的参数是什么，返回值是什么），服务的网络地址用哪个url地址表示，服务通过什么方式来调用。 WSDL(Web Services Description Language)就是这样一个基于XML的语言，用于描述Web Service及其函数、参数和返回值。它是WebService客户端和服务器端都 能理解的标准格式。因为是基于XML的，所以WSDL既是机器可阅读的，又是人可阅读的，这将是一个很大的好处。一些最新的开发工具既能根据你的 Web service生成WSDL文档，又能导入WSDL文档，生成调用相应WebService的代理类代码。 WSDL 文件保存在Web服务器上，通过一个url地址就可以访问到它。客户端要调用一个WebService服务之前，要知道该服务的WSDL文件的地址。 WebService服务提供商可以通过两种方式来暴露它的WSDL文件地址：1.注册到UDDI服务器，以便被人查找；2.直接告诉给客户端调用者。 四、WebService 开发 ​ WebService 开发可以分为服务器端开发和客户端开发两个方面 4.1、服务端开发 ​ 把公司内部系统的业务方法发布成 WebService 服务，供远程合作单位和个人调用。(借助一些 WebService 框架可以很轻松地吧自己的业务对象发布成 WebService 服务，Java 方面的典型 WebService 框架包括：axis，小fire，cxf 等，java ee 服务器通常也支持发布 WebService 服务，例如 JBOSS。) 4.2、客户端开发 ​ 调用别人发布的WebService服务，大多数人从事的开发都属于这个方面，例如，调用天气预报WebService服务。（使用厂 商的WSDL2Java之类的工具生成静态调用的代理类代码；使用厂商提供的客户端编程API类；使用SUN公司早期标准的jax-rpc开发包；使用SUN公司最新标准的jax-ws开发包。当然SUN已被ORACLE收购) 4.3、WebService 的工作调用原理 ​ 对客户端而言，我们给这各类WebService客户端API传递wsdl文件的url地址，这些API就会创建出底层的代理类，我调用这些代理，就可以访问到webservice服务。代理类把客户端的方法调用变成soap格式的请求数据再通过HTTP协议发出去，并把接收到的soap 数据变成返回值返回。对服务端而言，各类WebService框架的本质就是一个大大的Servlet，当远程调用客户端给它通过http协议发送过来soap格式的请求数据时，它分析这个数据，就知道要调用哪个java类的哪个方法，于是去查找或创建这个对象，并调用其方法，再把方法返回的结果包装成soap格式的数据，通过http响应消息回给客户端。 五、使用场景 1、跨防火墙通信 ​ 如果应用程序有成千上万的用户，而且分布在世界各地，那么客户端和服务器之间的通信将是一个棘手的问题。因为客户端和服务器之间通常会有防火墙或者代理服 务器。在这种情况下，使用DCOM就不是那么简单，通常也不便于把客户端程序发布到数量如此庞大的每一个用户手中。传统的做法是，选择用浏览器作为客户端，写下一大堆ASP页面，把应用程序的中间层暴露给最终用户。这样做的结果是开发难度大，程序很难维护。如果中间层组件换成WebService的话，就可以从用户界面直接调用中间层组件。从大多数人的经验来看，在一个用户界面和中间层有较多交互的应用程序中，使用WebService这种结构，可以节省花在用户界面编程上20%的开发时间。 2、应用程序集成 ​ 企业级的应用程序开发者都知道，企业里经常都要把用不同语言写成的、在不同平台上运行的各种程序集成起来，而这种集成将花费很大的开发力量。应用程序经常需要从运行在IBM主机上的程序中获取数据；或者把数据发送到主机或UNIX应用程序中去。即使在同一个平台上，不同软件厂商生产的各种软件也常常需要集成起来。通过WebService，可以很容易的集成不同结构的应用程序。 3、B2B 集成 用WebService集成应用程序，可以使公司内部的商务处理更加自动化。但当交易跨越供应商和客户、突破公司的界限时会怎么样呢？跨公司的商务交易集成通常叫做B2B集成。WebService是B2B集成成功的关键。通过WebService，公司可以把关键的商务应用“暴露”给指定的供应商和客户。例如，把电子下单系统和电子发票系统“暴露”出来，客户就可以以电子的方式发送订单，供应商则可以以电子的方式发送原料采购发票。当然，这并不是一个新的概念，EDI(电子文档交换)早就是这样了。但是，WebService的实现要比EDI简单得多，而且WebService运行在Internet上，在世界任何地方都可轻易实现，其运行成本就相对较低。不过，WebService并不像EDI那样，是文档交换或B2B集成的完整解决方案。 WebService只是B2B集成的一个关键部分，还需要许多其它的部分才能实现集成。 ​ 用WebService来实现B2B集成的最大好处在于可以轻易实现互操作性。只要把商务逻辑“暴露”出来，成为WebService，就可以让任何指定的合作伙伴调用这些商务逻辑，而不管他们的系统在什么平台上运行，使用什么开发语言。这样就大大减少了花在B2B集成上的时间和成本，让许多原本无法承受EDI的中小企业也能实现B2B集成。 4、软件和数据重用 ​ 软件重用是一个很大的主题，重用的形式很多，重用的程度有大有小。最基本的形式是源代码模块或者类一级的重用，一种形式是二进制形式的组件重用。采用 WebService应用程序可以用标准的方法把功能和数据“暴露”出来，供其它应用程序使用，达到业务级重用。 六、不适用场合 1、单机应用程序 ​ 目前，企业和个人还使用着很多桌面应用程序。其中一些只需要与本机上的其它程序通信。在这种情况下，最好就不要用WebService，只要用本地的API就可以了。COM非常适合于在这种情况下工作，因为它既小又快。运行在同一台服务器上的服务器软件也是这样。最好直接用COM或其它本地的API来进行应用程序间的调用。当然WebService也能用在这些场合，但那样不仅消耗太大，而且不会带来任何好处。 2、局域网的同构应用程序 在许多应用中，所有的程序都是用VB或VC开发的，都在Windows平台下使用COM，都运行在同一个局域网上。例如，有两个服务器应用程序需要相互通信，或者有一个Win32或WinForm的客户程序要连接局域网上另一个服务器的程序。在这些程序里，使用DCOM会比SOAP/HTTP有效得多。与此相类似，如果一个.NET程序要连接到局域网上的另一个.NET程序，应该使.NETremoting。有趣的是，在.NETremoting 中，也可以指定使用SOAP/HTTP来进行WebService调用。不过最好还是直接通过TCP进行RPC调用，那样会有效得多。 七、编码中的一些概念 1、Port Type ​ 事实上，一个 WebService 并不是直接包含一组 operation(方法)。方法是被组成换一个或多个 Port Type。 一个 Port Type 类似 Java 类，每个 operation 类似 java class 中的静态方法。 比如，一个web service中，把所有string相关操作组成 stringUtil Port type, 把日期相关的操作组成dateUtil Port Type. 所有 port type的命名必须是QName. （QName 就是需要有 namespace和localname的全名称， 见上篇的图示） 2、Binding 一个 port type 允许使用不同的信息格式访问，比如SOAP(Simple Object Access Protocal)或 普通文本格式(plain text fomat): concat(s1=‘abc’, s2=‘123’) 除了信息格式，每个port type还允许使用信息通过HTTP Post 请求或者 通过 email方式传送。 因此，每个被支持的信息格式和信息传送方式组合，就叫做 binding.最常见的binding就是 SOAP+HTTP. 3、Port 假如很多人使用你的web service,你决定把你的web service部署到3台机器上(C1,C2,C3)。部署策略为：采用binding1于C1,C2,C3 机器上；采用binding2于C3机器上.此时，我们就说，你一共有四个port, 其中3个port使用用binding1, 1个port使用binding2.","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"webservice","slug":"webservice","permalink":"https://jokinglove.com/blog/group/tags/webservice/"},{"name":"java","slug":"java","permalink":"https://jokinglove.com/blog/group/tags/java/"}]},{"title":"spring boot 中的坑","slug":"others/microservice/spring-boot-notice","date":"2019-06-24T16:00:00.000Z","updated":"2020-10-17T08:07:03.868Z","comments":true,"path":"2019/06/25/others/microservice/spring-boot-notice/","link":"","permalink":"https://jokinglove.com/blog/2019/06/25/others/microservice/spring-boot-notice/","excerpt":"","text":"spring boot 中注意的问题点 YAML 文件的缺点： YAML文件不能通过 @PropertySource 注解加载，如果需要使用该方式，那就必须使用properties文件。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"https://jokinglove.com/blog/group/tags/springboot/"}]},{"title":"Kubernetes-创建集群","slug":"kubernates/4.1创建kubernetes集群","date":"2019-06-13T16:00:00.000Z","updated":"2020-10-17T08:07:03.862Z","comments":true,"path":"2019/06/14/kubernates/4.1创建kubernetes集群/","link":"","permalink":"https://jokinglove.com/blog/2019/06/14/kubernates/4.1创建kubernetes集群/","excerpt":"使用 Minikube 创建一个集群 目标 了解什么是 Kubernetes 集群 了解什么是 Minikube 使用在线终端启动 Kubernetes 集群","text":"使用 Minikube 创建一个集群 目标 了解什么是 Kubernetes 集群 了解什么是 Minikube 使用在线终端启动 Kubernetes 集群 Kubernetes 集群 Kubernetes 用于协调高度可用的计算机集群，这些计算机集群被连接作为单个单元工作。 Kubernetes 的抽象性允许您将容器的应用程序部署到集群，而不必专门将其绑定到单个计算机。为了利用这种新的部署模型，应用程序需要以将他们与各个主机分离的方式打包：他们需要被容器化。容器化应用程序比过去的部署模型更灵活和可用，其中应用程序直接安装到特定机器上，作为深入集成到主机中的软件包。Kubernetes 在一个集群上以更有效的方式自动分发和调度容器应用程序。 Kubernetes 是一个开源平台，可满足生产环境的需要。 Kubernetes 集群有两种类型的资源组成： 一个 Master 是集群的调度节点 Nodes 是应用程序实际运行的工作节点 概要： Kubernetes 集群 MiniKube Kubernetes 是一个生产级的开源平台，用于协调计算机集群内部和跨计算机集群的应用程序容器分发（调度）和运行。 集群图 Master 负责管理集群。 master 协调集群中的所有活动，例如调度应用程序、维护应用程序的所需状态、扩展应用程序和滚动更新。 节点是 Kubernetes 集群中的工作机器，可以是物理机或者虚拟机。 每个工作节点都有一个 Kubelet， 它是管理节点并与 Kubernetes Master 几点进行通信的代理。节点上应该具有处理容器操作的工作，例如 Docker 或者 rkt。一个 Kubernetes 工作集群至少有三个节点。 Master 管理集群，而节点用于托管正在运行的应用程序。 当您在 Kubernetes 上部署应用程序时，您可以告诉 master 启动应用程序容器。 Master 调度容器在集群的节点上运行。节点使用 Master 公开的 Kubernetes API 与 Master 通信。 最终用户还可以直接使用 Kubernetes API 与集群交互。 Kubernetes 集群可以部署在物理机或者虚拟机上。要开始使用 Kubernetes 开发吗， 您可以使用 Minikube 。Minikube 是一个轻量级的 Kubernetes 实现，会在本机创建一台虚拟机，并部署一个只包含一个几点的简单集群。 Minikube 适用于 Linux、Mac Os 和 Windows 系统。Minikube CLI 提供了集群的基本引导操作，包括启动、停止、状态和删除。为了完成此基础训练，您可以使用预先安装了 Minikube 的在线终端。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/categories/kubernetes/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/tags/kubernetes/"},{"name":"集群","slug":"集群","permalink":"https://jokinglove.com/blog/group/tags/集群/"}]},{"title":"Kubernetes-实操","slug":"kubernates/4.2操作练习","date":"2019-06-11T16:00:00.000Z","updated":"2020-10-17T08:07:03.863Z","comments":true,"path":"2019/06/12/kubernates/4.2操作练习/","link":"","permalink":"https://jokinglove.com/blog/2019/06/12/kubernates/4.2操作练习/","excerpt":"实操练习 123456789101112131415161718192021222324252627282930313233Kubernetes Bootcamp Terminal$ minikube versionminikube version: v0.34.1$ minikube starto minikube v0.34.1 on linux (amd64)&gt; Configuring local host environment ...&gt; Creating none VM (CPUs=2, Memory=2048MB, Disk=20000MB) ...- \"minikube\" IP address is 172.17.0.7- Configuring Docker as the container runtime ...- Preparing Kubernetes environment ...@ Downloading kubeadm v1.13.3@ Downloading kubelet v1.13.3- Pulling images required by Kubernetes v1.13.3 ...- Launching Kubernetes v1.13.3 using kubeadm ...- Configuring cluster permissions ...- Verifying component health .....+ kubectl is now configured to use \"minikube\"= Done! Thank you for using minikube!$$ kubectl versionClient Version: version.Info&#123;Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:08:12Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125;Server Version: version.Info&#123;Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125;$$ kubectl cluster-infoKubernetes master is running at https://172.17.0.7:8443KubeDNS is running at https://172.17.0.7:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.$$ kubectl get nodesNAME STATUS ROLES AGE VERSIONminikube Ready master 100s v1.13.3","text":"实操练习 123456789101112131415161718192021222324252627282930313233Kubernetes Bootcamp Terminal$ minikube versionminikube version: v0.34.1$ minikube starto minikube v0.34.1 on linux (amd64)&gt; Configuring local host environment ...&gt; Creating none VM (CPUs=2, Memory=2048MB, Disk=20000MB) ...- \"minikube\" IP address is 172.17.0.7- Configuring Docker as the container runtime ...- Preparing Kubernetes environment ...@ Downloading kubeadm v1.13.3@ Downloading kubelet v1.13.3- Pulling images required by Kubernetes v1.13.3 ...- Launching Kubernetes v1.13.3 using kubeadm ...- Configuring cluster permissions ...- Verifying component health .....+ kubectl is now configured to use \"minikube\"= Done! Thank you for using minikube!$$ kubectl versionClient Version: version.Info&#123;Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:08:12Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125;Server Version: version.Info&#123;Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125;$$ kubectl cluster-infoKubernetes master is running at https://172.17.0.7:8443KubeDNS is running at https://172.17.0.7:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.$$ kubectl get nodesNAME STATUS ROLES AGE VERSIONminikube Ready master 100s v1.13.3 12","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/categories/kubernetes/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/tags/kubernetes/"}]},{"title":"Linux 实用命令","slug":"others/linux/20190610-linux常用命令","date":"2019-06-10T16:00:00.000Z","updated":"2020-10-17T08:07:03.867Z","comments":true,"path":"2019/06/11/others/linux/20190610-linux常用命令/","link":"","permalink":"https://jokinglove.com/blog/2019/06/11/others/linux/20190610-linux常用命令/","excerpt":"Linux 实用命令 awk awk 是一种处理文本文件的语言","text":"Linux 实用命令 awk awk 是一种处理文本文件的语言 12345678910111213141516171819202122232425262728293031awk [选项参数] 'script' var=value file(s)或awk [选项参数] -f scriptfile var=value file(s)选项参数说明：-F fs or --field-separator fs指定输入文件折分隔符，fs是一个字符串或者是一个正则表达式，如-F:。-v var=value or --asign var=value赋值一个用户定义变量。-f scripfile or --file scriptfile从脚本文件中读取awk命令。-mf nnn and -mr nnn对nnn值设置内在限制，-mf选项限制分配给nnn的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。-W compact or --compat, -W traditional or --traditional在兼容模式下运行awk。所以gawk的行为和标准的awk完全一样，所有的awk扩展都被忽略。-W copyleft or --copyleft, -W copyright or --copyright打印简短的版权信息。-W help or --help, -W usage or --usage打印全部awk选项和每个选项的简短说明。-W lint or --lint打印不能向传统unix平台移植的结构的警告。-W lint-old or --lint-old打印关于不能向传统unix平台移植的结构的警告。-W posix打开兼容模式。但有以下限制，不识别：/x、函数关键字、func、换码序列以及当fs是一个空格时，将新行作为一个域分隔符；操作符**和**=不能代替^和^=；fflush无效。-W re-interval or --re-inerval允许间隔正则表达式的使用，参考(grep中的Posix字符类)，如括号表达式[[:alpha:]]。-W source program-text or --source program-text使用program-text作为源代码，可与-f命令混用。-W version or --version打印bug报告信息的版本。 示例 12# 获取ps -aux 结果的所有PIDps -aux| awk '&#123;print $2&#125;' tNULSeBaMsA7pkcKvTbL6fYDGPLypFegvkyxeH tNULSeBaMnbevvimnwvfkF3tgW3SPrWf6nyEws","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://jokinglove.com/blog/group/tags/linux/"}]},{"title":"Kubernetes-基础模块","slug":"kubernates/4.kubernetes的基础模块","date":"2019-06-09T16:00:00.000Z","updated":"2020-10-17T08:07:03.863Z","comments":true,"path":"2019/06/10/kubernates/4.kubernetes的基础模块/","link":"","permalink":"https://jokinglove.com/blog/2019/06/10/kubernates/4.kubernetes的基础模块/","excerpt":"Kubernetes 的基础模块 创建一个 Kubernetes 集群 部署应用程序 应用程序探索 应用外部可见 应用可扩展 应用更新","text":"Kubernetes 的基础模块 创建一个 Kubernetes 集群 部署应用程序 应用程序探索 应用外部可见 应用可扩展 应用更新","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/categories/kubernetes/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/tags/kubernetes/"}]},{"title":"Kubernetes-设计架构","slug":"kubernates/3.kubernetes-设计架构","date":"2019-06-01T16:00:00.000Z","updated":"2020-10-17T08:07:03.862Z","comments":true,"path":"2019/06/02/kubernates/3.kubernetes-设计架构/","link":"","permalink":"https://jokinglove.com/blog/2019/06/02/kubernates/3.kubernetes-设计架构/","excerpt":"Kubernetes 设计架构 Kubernetes 集群包含有节点代理 kubelet 和 Master 组件 (APIs, scheduler, etc)，一切都基于分布式存储系统。下面这张图是 Kubernetes 的结构图。 查看高清无码图","text":"Kubernetes 设计架构 Kubernetes 集群包含有节点代理 kubelet 和 Master 组件 (APIs, scheduler, etc)，一切都基于分布式存储系统。下面这张图是 Kubernetes 的结构图。 查看高清无码图 Kubernetes 节点 在这张系统架构图中，我们把服务分为运行在工作节点上的服务和组成集群级别控制板的服务。 Kubernetes 节点有运行应用容器必备的服务，而这些都是受 Master 的控制。 每个节点上都要运行 Docker。Docker 来负责所有具体的镜像下载和容器运行。 Kubernetes 主要由以下几个核心组件组成： etcd 保存了整个集群的状态； apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制； controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上； kubelet 负责维护容器的生命周期，同时也负责 Volume (CVI) 和网路 (CNI) 的管理； Container runtime 负责镜像管理以及 Pod 和容器的真正运行 (CRI)； kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡； 除了核心组件，还有一些推荐的 Add-ons： kube-dns 负责为整个集群提供 DNS 服务 Ingress Controller 为服务提供外网入口 Heapster 提供资源监控 Dashboard 提供 GUI Federation 提供跨可用区的集群 Fluentd-elasticsearch 提供集群日志采集、存储与查询 分层架构 Kubernetes 设计理念和功能其实就是一个类 Linux 的分层架构，如下图所示： 核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境 应用层：部署 (无状态应用、有状态应用、批处理任务、集群应用等) 和路由 (服务发现、DNS解析等) 管理层：系统度量 (如基础设施、容器和网络的度量) ，自动化 (如自动扩展、动态 Provision 等) 以及策略管理 (RBAC、Quota、 PSP、NetworkPolicy等) 接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴： Kubernetes 外部：日志、监控、配置管理、CI、CD、Workflow、Faas、OTS应用、ChatOps 等 Kubernetes 内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等 kubelet kubelet 负责管理 pods 和他们上面的容器，image 镜像、volumes、etc。 kube-proxy 每一个节点运行一个简单的网络代理和负载均衡 ( 详见 services FAQ ) 。正如 Kubernetes API 里面定义的这些服务 ( 详见 the services doc ) 也可以在各种终端中以轮询的方式做一些简单的 TCP 和 UDP 传输。 服务端点目前是通过 DNS 或者环境变量 ( Docker-links-compatible 和 Kubernetes{FOO}_SERVICE_HOST 及 {FOO}_SERVICE_PORT 变量都支持）。这些变量由服务代理所管理的端口来解析。 Kubernetes 控制面板 Kubernetes 控制面板可以分为多个部分。目前他们都运行在一个 master 节点，然而为了达到高可用性，这需要改变。不同部分一起协作提供一个统一的关于集群的视图。 etcd 所有 master 的持续状态都在 etcd 的一个实例中。这可以很好地存储配置数据。因为有 watch (观察者) 的支持，个部件协调中的改变可以很快被察觉。 Kubernetes API Server API 服务提供 Kubernetes API 的服务。这个服务试图通过吧所有或者大部分的业务逻辑放到不两只的部件中从而使其具有 CRUD 特性。它主要处理 REST 操作，在 etcd 中验证更新这些对象 (并最终存储)。 scheduler 调度器吧未调度的 pod 通过 binding api 绑定到节点上。调度器是可插拔的。并且我们期待支持多集群的调度，未来甚至希望可以支持用户自定义的调度。 Kubernetes 控制管理服务器 所有其他的集群级别的功能目前都是由控制管理器所负责。例如，端点对象是被端点控制器来创建和更新。这些最终可以被分隔成不同的部件来让他们独自的可插拔。 replicationscontroller 是一种建立于简单的 pod API 之上的一种机制。一旦实现，我们最终计划把这变成一种通用的插件机制。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/categories/kubernetes/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/tags/kubernetes/"}]},{"title":"Kubernetes-认识 Kubernetes","slug":"kubernates/2.kubernetes-认识 Kubernetes","date":"2019-05-30T16:00:00.000Z","updated":"2020-10-17T08:07:03.862Z","comments":true,"path":"2019/05/31/kubernates/2.kubernetes-认识 Kubernetes/","link":"","permalink":"https://jokinglove.com/blog/2019/05/31/kubernates/2.kubernetes-认识 Kubernetes/","excerpt":"认识 Kubernetes？ Kubernetes 具有如下特点： 准备好 开始? 为什么是容器？ 为什么我们需要 Kubernetes，它能做什么？ 为什么 Kubernetes 是一个平台？ Kubernetes 不是什么： Kubernetes 是什么意思？K8s？ Kubernetes 是一个跨主机集群的 开源的容器调度平台，它可以自动化应用容器的部署、扩展和操作 ，提供以容器为中心的基础架构。 使用 Kubernetes ， 你可以快速高效地相应客户的需求： 快速、可预测地部署你的应用程序 拥有即时扩展应用程序的能力 不影响现有业务的情况下，无缝的发布新功能 优化硬件资源，降低成本 我们的目标是构建一个软件和工具的生态系统，以减轻你在公共云或私有云运行应用程序的负担。","text":"认识 Kubernetes？ Kubernetes 具有如下特点： 准备好 开始? 为什么是容器？ 为什么我们需要 Kubernetes，它能做什么？ 为什么 Kubernetes 是一个平台？ Kubernetes 不是什么： Kubernetes 是什么意思？K8s？ Kubernetes 是一个跨主机集群的 开源的容器调度平台，它可以自动化应用容器的部署、扩展和操作 ，提供以容器为中心的基础架构。 使用 Kubernetes ， 你可以快速高效地相应客户的需求： 快速、可预测地部署你的应用程序 拥有即时扩展应用程序的能力 不影响现有业务的情况下，无缝的发布新功能 优化硬件资源，降低成本 我们的目标是构建一个软件和工具的生态系统，以减轻你在公共云或私有云运行应用程序的负担。 Kubernetes 具有如下特点： 便携性：无论公有云、私有云、混合云还是多云架构都全面支持 可扩展：它是模块化、可插拔、可挂载、可组合的，支持各种形式的扩展 自修复： 它可以自我保持应用的状态、可自重启、自复制、自缩放的，通过声明式语法提供了强大的自我修复能力 Kubernetes 项目有 Google 公司在 2014 年启动。 Kubernetes 建立在 Goole 公司超过十余年的运维经验基础之上，Google 所有的应用都运行在容器上，再与社区中最好的想法和实践相结合，也许他是最受欢迎的容器平台。 为什么是容器？ 传统部署应用程序的方式，一般是使用操作系统自带的包管理器在主机上安装应用依赖，之后再安装应用程序。这无疑将应用程序的可执行文件、应用的配置、应用依赖库和应用的生命周期与宿主机操作系统进行了紧耦合。在此情境下，可以通过构建不可改变的虚拟机镜像版本，通过镜像版本实现可预测的发布和回滚，但是虚拟机实在是太重量级了，且镜像体积太庞大，便携性差。 新方式 是基于操作系统级虚拟化而不是硬件级虚拟化方法来部署容器。容器之间彼此隔离并与主机隔离：他们具有自己的文件系统，不能看到彼此的进程，并且他们所使用的计算机资源是可以被限制的。他们比虚拟机更容易构建，并且他们与底层基础架构和主机文件系统隔离，所以他们可以跨云和操作系统快速分发。 由于容器体积小且启动快，因此可以在每个容器镜像中打包一个应用程序。这种一对一的应用镜像关系拥有很多好处。使用容器，不需要与外部的基础架构环境绑定，因为每一个应用程序都不需要外部依赖，更不需要与外部基础架构环境依赖。完美的解决了从开发到生产环境的一致性问题。 容器同样比虚拟机更加透明，这有助于监测和管理。尤其是容器进程的生命周期由基础设施管理，而不是由容器内的进程对外隐藏时更是如此。最后，每个应用程序用容器封装，管理容器部署就等同于管理应用程序部署。 容器优点摘要： 敏捷的应用程序创建和部署：与虚拟机镜像相比，容器镜像更容易创建，提升了硬件的使用效率。 持续开发、集成和部署：提供可靠与频繁的容器镜像构建和部署，可以很方便以及快速的回滚 (由于镜像不可变性)。 关注开发与运维的分离：在构建/发布时创建应用程序容器镜像，从而将应用程序与基础架构分离。 开发、测试和生产环境的一致性：在笔记本电脑上运行与云中一样。 云和操作系统的可移植性：可运行在 Ubuntu， RHEL，CoreOS，内部部署， Google 容器引擎和其他任何地方。 松耦合、分布式、弹性伸缩_微服务_: 应用程序被分成更小，更独立的部分，可以动态部署和管理 - 而不是巨型单体应用运行在专用的大型机上。 资源隔离：通过对应用进行资源隔离，可以很容易的预测应用程序性能。 资源利用：高效率和高密度 为什么我们需要 Kubernetes， 它能做什么？ 最基础的，Kubernetes 可以在物理或虚拟机集群上调度和运行应用程序容器。然而，Kubernetes 还允许开发人员从物理和虚拟机 “脱离”， 从以主机为中心的基础架构转义到以容器为中心的基础架构，这样可以提供容器固有的全部优点和益处。Kubernetes 提供了基础设施来构建一个真正以容器为中心的开发环境。 Kubernetes 满足了生产中运行应用程序的许多常见的需求，例如： Pod 提供符合应用并保留一个应用一个容器的容器模型。 挂载外部存储 Secret管理 应用健康检查 副本应用实例 横向自动扩容 服务发现 负载均衡 滚动更新 资源监测 日志采集和存储 支持自检和调试 认证和鉴权 这提供了平台即服务 (PASS) 的简单性以及基础架构即服务 (IAAS) 的灵活性，并促进跨基础设施供应商的可移植性。 为什么 Kubernetes 是一个平台？ Kubernetes 提供了很多的功能，总会有新的场景受益于新特性。它可以简化应用程序的工作流，加快开发速度。被大家认可的应用编排通常需要有较强的自动化能力。这就是为什么 Kubernetes 被设计作为构建组件和工具的生态系统平台，以便轻松地部署、扩展和管理应用程序。 Label 允许用户按照自己的方式组织管理对应的资源。Annotation 使用户能够以自定义的描述信息来修饰资源，以适用自己的工作流，并为管理工具提供检查点状态的简单方法。 Edit This Page 认识 Kubernetes? Kubernetes 具有如下特点:准备好 开始?为什么是容器?为什么我们需要 Kubernetes，它能做什么?为什么 Kubernetes 是一个平台?Kubernetes 不是什么:Kubernetes 是什么意思? K8s? Kubernetes 是一个跨主机集群的 开源的容器调度平台，它可以自动化应用容器的部署、扩展和操作 , 提供以容器为中心的基础架构。 使用 Kubernetes, 您可以快速高效地响应客户需求: 快速、可预测地部署您的应用程序 拥有即时扩展应用程序的能力 不影响现有业务的情况下，无缝地发布新功能 优化硬件资源，降低成本 我们的目标是构建一个软件和工具的生态系统，以减轻您在公共云或私有云运行应用程序的负担。 Kubernetes 具有如下特点: 便携性: 无论公有云、私有云、混合云还是多云架构都全面支持 可扩展: 它是模块化、可插拔、可挂载、可组合的，支持各种形式的扩展 自修复: 它可以自保持应用状态、可自重启、自复制、自缩放的，通过声明式语法提供了强大的自修复能力 Kubernetes 项目由 Google 公司在 2014 年启动。Kubernetes 建立在 Google 公司超过十余年的运维经验基础之上，Google 所有的应用都运行在容器上, 再与社区中最好的想法和实践相结合，也许它是最受欢迎的容器平台。 准备好 开始? 为什么是容器? 查看此文，可以了解为什么您要使用容器 容器? 传统 部署应用程序的方式，一般是使用操作系统自带的包管理器在主机上安装应用依赖，之后再安装应用程序。这无疑将应用程序的可执行文件、应用的配置、应用依赖库和应用的生命周期与宿主机操作系统进行了紧耦合。在此情境下，可以通过构建不可改变的虚拟机镜像版本，通过镜像版本实现可预测的发布和回滚，但是虚拟机实在是太重量级了，且镜像体积太庞大，便捷性差。 新方式 是基于操作系统级虚拟化而不是硬件级虚拟化方法来部署容器。容器之间彼此隔离并与主机隔离：它们具有自己的文件系统，不能看到彼此的进程，并且它们所使用的计算资源是可以被限制的。它们比虚拟机更容易构建，并且因为它们与底层基础架构和主机文件系统隔离，所以它们可以跨云和操作系统快速分发。 由于容器体积小且启动快，因此可以在每个容器镜像中打包一个应用程序。这种一对一的应用镜像关系拥有很多好处。使用容器，不需要与外部的基础架构环境绑定, 因为每一个应用程序都不需要外部依赖，更不需要与外部的基础架构环境依赖。完美解决了从开发到生产环境的一致性问题。 容器同样比虚拟机更加透明，这有助于监测和管理。尤其是容器进程的生命周期由基础设施管理，而不是由容器内的进程对外隐藏时更是如此。最后，每个应用程序用容器封装，管理容器部署就等同于管理应用程序部署。 容器优点摘要: 敏捷的应用程序创建和部署: 与虚拟机镜像相比，容器镜像更容易创建，提升了硬件的使用效率。 持续开发、集成和部署: 提供可靠与频繁的容器镜像构建和部署，可以很方便及快速的回滚 (由于镜像不可变性). 关注开发与运维的分离: 在构建/发布时创建应用程序容器镜像，从而将应用程序与基础架构分离。 开发、测试和生产环境的一致性: 在笔记本电脑上运行与云中一样。 云和操作系统的可移植性: 可运行在 Ubuntu, RHEL, CoreOS, 内部部署, Google 容器引擎和其他任何地方。 以应用为中心的管理: 提升了操作系统的抽象级别，以便在使用逻辑资源的操作系统上运行应用程序。 松耦合、分布式、弹性伸缩 微服务: 应用程序被分成更小，更独立的部分，可以动态部署和管理 - 而不是巨型单体应用运行在专用的大型机上。 资源隔离: 通过对应用进行资源隔离，可以很容易的预测应用程序性能。 资源利用: 高效率和高密度。 为什么我们需要 Kubernetes，它能做什么? 最基础的，Kubernetes 可以在物理或虚拟机集群上调度和运行应用程序容器。然而，Kubernetes 还允许开发人员从物理和虚拟机’脱离’，从以主机为中心的基础架构转移到以容器为中心的基础架构，这样可以提供容器固有的全部优点和益处。Kubernetes 提供了基础设施来构建一个真正以容器为中心的开发环境。 Kubernetes 满足了生产中运行应用程序的许多常见的需求，例如： Pod 提供复合应用并保留一个应用一个容器的容器模型, 挂载外部存储, Secret管理, 应用健康检查, 副本应用实例, 横向自动扩缩容, 服务发现, 负载均衡, 滚动更新, 资源监测, 日志采集和存储, 支持自检和调试, 认证和鉴权. 这提供了平台即服务 (PAAS) 的简单性以及基础架构即服务 (IAAS) 的灵活性，并促进跨基础设施供应商的可移植性。 有关详细信息，请参阅 用户指南. 为什么 Kubernetes 是一个平台? Kubernetes 提供了很多的功能，总会有新的场景受益于新特性。它可以简化应用程序的工作流，加快开发速度。被大家认可的应用编排通常需要有较强的自动化能力。这就是为什么 Kubernetes 被设计作为构建组件和工具的生态系统平台，以便更轻松地部署、扩展和管理应用程序。 Label 允许用户按照自己的方式组织管理对应的资源。 注解 使用户能够以自定义的描述信息来修饰资源，以适用于自己的工作流，并为管理工具提供检查点状态的简单方法。 此外，Kubernetes 控制面 (Control Plane) 是构建在相同的 APIs 上面，开发人员和用户都可以用。用户可以编写自己的控制器， 调度器等等，如果这么做，根据新加的自定义 API ，可以扩展当前的通用 CLI 命令行工具。 这种 设计 使得许多其他系统可以构建在 Kubernetes 之上。 Kubernetes 不是什么： Kubernetes 不是一个传统意义上，包罗万象的 PaaS (平台即服务) 系统。我们保留用户选择的自由，这非常重要。 Kubernetes 不限制支持的应用程序类型。 它不插手应用程序框架 (例如 Wildfly), 不限制支持的语言运行时 (例如 Java, Python, Ruby)，只迎合符合 12种因素的应用程序，也不区分”应用程序”与”服务”。Kubernetes 旨在支持极其多样化的工作负载，包括无状态、有状态和数据处理工作负载。如果应用可以在容器中运行，它就可以在 Kubernetes 上运行。 Kubernetes 不提供作为内置服务的中间件 (例如 消息中间件)、数据处理框架 (例如 Spark)、数据库 (例如 mysql)或集群存储系统 (例如 Ceph)。这些应用可以运行在 Kubernetes 上。 Kubernetes 没有提供点击即部署的服务市场。 Kubernetes 从源代码到镜像都是非垄断的。 它不部署源代码且不构建您的应用程序。 持续集成 (CI) 工作流是一个不同用户和项目都有自己需求和偏好的领域。 所以我们支持在 Kubernetes 分层的 CI 工作流，但不指定它应该如何工作。 Kubernetes 允许用户选择其他的日志记录，监控和告警系统 (虽然我们提供一些集成作为概念验证) Kubernetes 不提供或授权一个全面的应用程序配置语言/系统 (例如 jsonnet). Kubernetes 不提供也不采用任何全面机器配置、保养、管理或自我修复系统 另一方面，许多 PaaS 系统运行 在 Kubernetes 上面，例如 Openshift, Deis, and Eldarion。 您也可以自定义您自己的 PaaS, 与您选择的 CI 系统集成，或与 Kubernetes 一起使用: 将您的容器镜像部署到 Kubernetes。 由于 Kubernetes 在应用级别而不仅仅在硬件级别上运行，因此它提供 PaaS 产品通用的一些功能，例如部署、扩展、负载均衡、日志记录、监控等。但是，Kubernetes 不是单一的，默认解决方案是可选和可插拔的。 此处，Kubernetes 不仅仅是一个 “编排系统”；它消除了编排的需要。 “编排”技术定义的是工作流的执行: 从 A 到 B，然后到 C。相反，Kubernetes 是包括一套独立、可组合的控制过程，通过声明式语法使其连续地朝着期望状态驱动当前状态。 不需要告诉它具体从 A 到 C 的过程，只要告诉到 C 的状态即可。 也不需要集中控制；该方法更类似于”编舞”。这使得系统更容易使用并且更强大、更可靠、更具弹性和可扩展性。 Kubernetes 是什么意思？K8s? 名称 Kubernetes 源于希腊语，意为 “舵手” 或 “飞行员” ，且是应为 “governor” 和 “cybernetic” 的词根。K8s 是通过将 8 个字母 “ubernete” 替换为 8 而导出的缩写。另外，在中文里，k8s 的发音与 Kubernetes 的发音比较接近。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/categories/kubernetes/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/tags/kubernetes/"}]},{"title":"Kubernetes-概念","slug":"kubernates/1.kubernetes-概念","date":"2019-05-27T16:00:00.000Z","updated":"2020-10-17T08:07:03.862Z","comments":true,"path":"2019/05/28/kubernates/1.kubernetes-概念/","link":"","permalink":"https://jokinglove.com/blog/2019/05/28/kubernates/1.kubernetes-概念/","excerpt":"概念 概念部分可以帮助你了解 Kubernetes 的各个组成部分以及 Kubernetes 用来表示集群的一些抽象概念，并帮助你更加深入的理解 Kubernetes 是如何工作的。 概述 [ Kubernetes 对象 ](# Kubernetes 对象) [Kubernetes 控制面](#Kubernetes 控制面) 接下来","text":"概念 概念部分可以帮助你了解 Kubernetes 的各个组成部分以及 Kubernetes 用来表示集群的一些抽象概念，并帮助你更加深入的理解 Kubernetes 是如何工作的。 概述 [ Kubernetes 对象 ](# Kubernetes 对象) [Kubernetes 控制面](#Kubernetes 控制面) 接下来 概述 要使用 Kubernetes，你需要使用 Kubernates API 对象来描述集群的预期状态 (desired state) ：包括你需要运行的应用或者负载，它们使用的镜像、副本数，以及所需网络和磁盘资源等等。你可以使用命令行工具 kubectl 来调用 Kubernetes API 创建对象，通过所创建的这些对象来配置预期状态。你也可以直接调用 Kubernetes API 和集群进行交互，设置或者修改预期状态。 一旦你设置了所需的目标状态， Kubernetes 控制面 (control plane) 会促成集群的当前状态符合其预期状态。为此， Kubernetes 会自动执行各类任务，比如运行或者重启容器、调整给定应用的副本数等等。 Kubernetes 控制面由一组运行在集群上的进程组成： Kubernetes 主控组件（Master） 包含三个进程，都运行在集群中的某个节上，通常这个节点被称为 master 节点。这些进程包括：kube-apiserver、kube-controller-manager 和 kube-scheduler。 集群中的每个非 master 节点都运行两个进程： kubelet, 和 master 节点进行通信。 kube-proxy, 一种网络代理，将 Kubernetes 的网络服务代理到每个节点上。 Kubernetes 对象 Kubernetes 包含若干抽象用来表示系统状态，包括：已部署的容器化应用和负载、与它们相关的网络和磁盘资源以及有关集群运行的其他操作信息。这些抽象使用 Kubernetes API 对象来表示。参阅 Kubernetes 对象概述 来了解详细信息。 基本的 Kubernetes 对象包括： * [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) * [Service](https://kubernetes.io/docs/concepts/services-networking/service/) * [Volume](https://kubernetes.io/docs/concepts/storage/volumes/) * [Namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) 另外， Kubernetes 包含大量的被称作 控制器 (controllers) 高级抽象。控制器基于基本对象构建并提供额外的功能和方便使用的特性。具体包括： ReplicaSet Deployment StatefulSet DaemonSet Job Kubernetes 控制面 关于 Kubernetes 控制平面的各个部分， 如 Kubernetes 主控组件和 kubelet 进程，管理着 Kubernetes 如何与你的集群进行通信。控制平面维护着系统中所有的 Kubernetes 对象的状态记录，并且通过连续的控制循环来管理这些对象的状态。在任一的给定时间点，控制面的控制环都能相应集群中的变化，并且让系统中所有对象的实际状态与你提供的预期状态项匹配。 比如，当你通过 Kubernetes API 创建一个 Deployment 对象，你就为系统增加了一个新的目标状态。 Kubernetes 控制平面记录着对象的创建，并启动必要的应用然后将他们调度至集群的某个节点上来执行你的指令，以此来保持集群的实际状态和目标状态的匹配。 Kubernetes Master 节点 Kubernetes master 节点负责维护集群的目标状态。当你与 Kubernetes 通信时，使用如 kubectl 的命令行工具，就可以直接与 Kubernetes master 节点进行通信。 “master” 是指管理集群状态的一组进程的集合。通常这些进程都跑在集群中的一个单独的节点上，并且这个几点被称为 master 几点。 master 节点也可以扩展副本数，来获取更好的性能及冗余。 Kubernates Node 节点 集群中的 node 节点 (虚拟机、物理机等等) 都是用来运行你的应用和云工作流的机器。 Kubernetes master 节点控制所有 node 节点；你很少需要和 node 节点进行直接通信。 对象元数据 注释 接下来","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/categories/kubernetes/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/tags/kubernetes/"}]},{"title":"Kafka Introduction","slug":"others/kafka/20190612-kafka-introduction","date":"2019-05-18T16:00:00.000Z","updated":"2020-10-17T08:07:03.866Z","comments":true,"path":"2019/05/19/others/kafka/20190612-kafka-introduction/","link":"","permalink":"https://jokinglove.com/blog/2019/05/19/others/kafka/20190612-kafka-introduction/","excerpt":"Introduction ​ Apache Kafka 是一个分布式流处理平台。这到底意味着什么呢？ 流处理平台的三种特性： 可以让你发布和订阅流式记录。这一方面与消息队列或者企业消息系统类似。 可以存储流式记录，并且有较好的容错性。 可以在流式记录产生时就进行处理。 Kafka 适合什么样的场景？ 它可以用于两大类别的应用： 构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。（相当于 message queue） 构建实时流式应用程序，对这些流式数据进行转换或者影响。（就是流处理，通过 Kafka stream topic 和 topic 之间内部进行变化）","text":"Introduction ​ Apache Kafka 是一个分布式流处理平台。这到底意味着什么呢？ 流处理平台的三种特性： 可以让你发布和订阅流式记录。这一方面与消息队列或者企业消息系统类似。 可以存储流式记录，并且有较好的容错性。 可以在流式记录产生时就进行处理。 Kafka 适合什么样的场景？ 它可以用于两大类别的应用： 构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。（相当于 message queue） 构建实时流式应用程序，对这些流式数据进行转换或者影响。（就是流处理，通过 Kafka stream topic 和 topic 之间内部进行变化） 为了理解 Kafka 是如何做到以上所说的功能，从下面开始，我们将深入探索 Kafka 的特性。 首先是一些概念： Kafka 作为一个集群，运行在一台或者多台服务器上。 Kafka 通过 topic 对存储的流数据进行分类。 每条记录中包含一个 key， 一个 value 和一个 timestamp （时间戳）。 Kafka 有四个核心的 API: Producer API 允许一个应用程序发布一串流式的数据到一个或者多个 Kafka topic。 Consumer API 允许一个应用程序订阅一个或者多个 topic ，并且对发布给他们的流式数据进行处理。 Stream API 允许一个应用程序作为一个流处理器，消费一个或者多个 topic 产生的输入流，然后生产一个输出流到一个或者多个 topic 中去，在输入输出流中进行有效的转换。 Connector API 允许构建并运行可重用的生产者或者消费者，将 Kafka topics 连接到已经存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。 在 Kafka 中，客户端和服务器使用一个简单、高性能、支持多语言的 TCP 协议。此协议版本化并且向下兼容老版本，我们为 Kafka 提供了 Java 客户端，也支持许多其他语言的客户端。 Topic和日志 让我们首先深入了解下 Kafka 的核心概念：提供一串流式记录 - topic Topic 就是数据主题，是数据记录发布的地方，可以用来区分业务系统。Kafka 中的 Topics 总是多订阅者模式，一个 topic 可以拥有一个或者多个消费者来订阅它的数据。 对于每一个 topic，Kafka 集群都会维持一个分区日志，如图所示： 每个分区都是有序且顺序不可变的记录集，并且不断地追加到结构化的 commit log 文件。分区中的每一个记录都会分配一个 id 号来表示顺序，我们称之为 offset，offset 用来唯一的表示分区中的每一条记录。 Kafka 集群保留所有发布的记录-无论他们是否已被消费-并通过一个可配置的参数–保留期限来控制。举个例子，如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。 __Kafka 的性能和数据大小无关，__所以长时间存储数据没什么问题。 事实上，在每一个消费者中唯一保存的元数据是 offset （偏移量）即消费者在 log 中的位置。偏移量由消费者所控制：通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从&quot;现在&quot; 开始消费。 这些细节说明 Kafka 消费者是非常廉价的 - 消费者的增加和减少吗，对集群或者其他消费者没有多大的影响。比如， 你可以使用命令行工具，对一些 topic 内容执行 tail 操作，并不会影响已存在的消费者消费数据。 日志中的 partition（分区）有以下几个用户： 当日志大小超过了单台服务器的限制，允许日志进行扩展。每个单独的分区都必须受限于主机的文件限制，不过一个主题可能有多个分区，因此可以处理无限量的数据。 可以作为并行的单元集 - 关于这一点，更多细节如下： 分布式 日志的分区 partition 分布在 Kafka 集群的服务器上。每个服务器在处理数据和请求时，共享这些分区。每一个分区都会在以配置的服务器上进行备份，确保容错性。 每个分区都有一台 server 作为 “leader“，零台或者多台 server 作为 followers。leader server 处理一切对 partition 的读写请求，而 followers 只需被动的同步 leader 上的数据。当 leader 宕机了，followers 中的服务器会自动成为行的 leader 。每台 server 都会成为某些分区的 leader 和某些分区的 followers ，因此集群的负载是平衡的。 生产者 生产者可以将数据发布到所选择的topic（主题）中。生产者负责将记录分配到topic的哪一个 partition（分区）中。可以使用循环的方式来简单地实现负载均衡，也可以根据某些语义分区函数(例如：记录中的key)来完成。下面会介绍更多关于分区的使用。 消费者 消费者使用一个 消费组 名称来进行标识，发布到topic中的每条记录被分配给订阅消费组中的一个消费者实例.消费者实例可以分布在多个进程中或者多个机器上。 如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例. 如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程. 如图，这个 Kafka 集群有两台 server 的，四个分区(p0-p3)和两个消费者组。消费组A有两个消费者，消费组B有四个消费者。 通常情况下，每个 topic 都会有一些消费组，一个消费组对应一个&quot;逻辑订阅者&quot;。一个消费组由许多消费者实例组成，便于扩展和容错。这就是发布和订阅的概念，只不过订阅者是一组消费者而不是单个的进程。 在Kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个实例都是分区唯一的消费者。维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例。 Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。每个 partition 分区按照key值排序足以满足大多数应用程序的需求。但如果你需要总记录在所有记录的上面，可使用仅有一个分区的主题来实现，这意味着每个消费者组只有一个消费者进程。 保证 high-level Kafka 给予以下保证： 生产者发送到特定 topic partition 的消息将按照发送的顺序处理。也就是说，如果记录 M1 和记录 M2 由相同的生产者发送，并先发送 M1 记录，那么 M1 的偏移比 M2 笑，并在日志中较早出现 一个消费者实例按照日志中的顺序查看记录 对于具有N个副本的主题，我们最多容忍 N-1 个服务器故障，从而保证不会丢失任何提交到日志中的记录。 关于保证的更多细节可以看文档中的设计部分。 Kafka 作为消息系统 Kafka streams的概念与传统的企业消息系统相比如何？ 传统的消息系统有两个模块: 队列 和 发布-订阅。 在队列中，消费者池从server读取数据，每条记录被池子中的一个消费者消费; 在发布订阅中，记录被广播到所有的消费者。两者均有优缺点。 队列的优点在于它允许你将处理数据的过程分给多个消费者实例，使你可以扩展处理过程。 不好的是，队列不是多订阅者模式的—一旦一个进程读取了数据，数据就会被丢弃。 而发布-订阅系统允许你广播数据到多个进程，但是无法进行扩展处理，因为每条消息都会发送给所有的订阅者。 消费组在Kafka有两层概念。在队列中，消费组允许你将处理过程分发给一系列进程(消费组中的成员)。 在发布订阅中，Kafka允许你将消息广播给多个消费组。 Kafka的优势在于每个topic都有以下特性—可以扩展处理并且允许多订阅者模式—不需要只选择其中一个. Kafka相比于传统消息队列还具有更严格的顺序保证 传统队列在服务器上保存有序的记录，如果多个消费者消费队列中的数据， 服务器将按照存储顺序输出记录。 虽然服务器按顺序输出记录，但是记录被异步传递给消费者， 因此记录可能会无序的到达不同的消费者。这意味着在并行消耗的情况下， 记录的顺序是丢失的。因此消息系统通常使用“唯一消费者”的概念，即只让一个进程从队列中消费， 但这就意味着不能够并行地处理数据。 Kafka 设计的更好。topic中的partition是一个并行的概念。 Kafka能够为一个消费者池提供顺序保证和负载平衡，是通过将topic中的partition分配给消费者组中的消费者来实现的， 以便每个分区由消费组中的一个消费者消耗。通过这样，我们能够确保消费者是该分区的唯一读者，并按顺序消费数据。 众多分区保证了多个消费者实例间的负载均衡。 但请注意，消费者组中的消费者实例个数不能超过分区的数量。 Kafka 作为存储系统 许多消息队列可以发布消息，除了消费消息之外还可以充当中间数据的存储系统。那么Kafka作为一个优秀的存储系统有什么不同呢? 数据写入Kafka后被写到磁盘，并且进行备份以便容错。直到完全备份，Kafka才让生产者认为完成写入，即使写入失败Kafka也会确保继续写入 Kafka使用磁盘结构，具有很好的扩展性—50kb和50TB的数据在server上表现一致。 可以存储大量数据，并且可通过客户端控制它读取数据的位置，您可认为Kafka是一种高性能、低延迟、具备日志存储、备份和传播功能的分布式文件系统。 关于Kafka提交日志存储和备份设计的更多细节，可以阅读 这页 。 Kafka 用做流处理 Kafka 流处理不仅仅用来读写和存储流式数据，它最终的目的是为了能够进行实时的流处理。 在Kafka中，流处理器不断地从输入的topic获取流数据，处理数据后，再不断生产流数据到输出的topic中去。 例如，零售应用程序可能会接收销售和出货的输入流，经过价格调整计算后，再输出一串流式数据。 简单的数据处理可以直接用生产者和消费者的API。对于复杂的数据变换，Kafka提供了Streams API。 Stream API 允许应用做一些复杂的处理，比如将流数据聚合或者join。 这一功能有助于解决以下这种应用程序所面临的问题：处理无序数据，当消费端代码变更后重新处理输入，执行有状态计算等。 Streams API建立在Kafka的核心之上：它使用Producer和Consumer API作为输入，使用Kafka进行有状态的存储， 并在流处理器实例之间使用相同的消费组机制来实现容错。 批处理 将消息、存储和流处理结合起来，使得 Kafka 看上去不一般，但这是它作为流平台所备的。 像 HDFS 这样的分布式文件系统可以存储用于批处理的静态文件。一个系统如果可以存储和处理历史数据是非常不错的。 传统的企业消息系统允许处理订阅后到达的数据。以这种方式来构建应用程序，并用它来处理即将到达的数据。 Kafka 结合了上面所说的两种特性。作为一个流应用程序平台或者流数据管道，这两个特性，对于 Kafka 来说是至关重要的。 通过组合存储和低延迟订阅，流式应用程序可以以同样的方式处理过去和未来的数据。一个单一的应用程序可以处理历史记录的数据，并且可以持续不断地处理以后到达的数据，而不是在到达最后一条记录时结束进程。这是一个广泛的流处理概念，其中包含批处理以及消息驱动应用程序。 同样，作为数据管道，能够订阅实时事件使得 Kafka 具有非常低的延迟；同时 Kafka 还具有可靠存储数据的特性，可用来储存重要的支付数据，或者与离线系统进行交互，系统可间歇性地加载数据，也可在停机维护后再次加载数据。流处理功能使得数据可以在到达时转换数据。 有关 Kafka 提供的保证、API 和功能的更多信息，请看文档的剩余部分。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"Kafka","slug":"Kafka","permalink":"https://jokinglove.com/blog/group/tags/Kafka/"},{"name":"大数据","slug":"大数据","permalink":"https://jokinglove.com/blog/group/tags/大数据/"}]},{"title":"MongoDb 常用方法","slug":"others/database/20190624-mongo常用方法","date":"2019-05-16T16:00:00.000Z","updated":"2020-10-17T08:07:03.864Z","comments":true,"path":"2019/05/17/others/database/20190624-mongo常用方法/","link":"","permalink":"https://jokinglove.com/blog/2019/05/17/others/database/20190624-mongo常用方法/","excerpt":"MongoDb 常用方法 $regx: 值匹配 123&#123; &lt;field&gt;: &#123; $regex: /pattern/, $options: '&lt;options&gt;' &#125; &#125;&#123; &lt;field&gt;: &#123; $regex: 'pattern', $options: '&lt;options&gt;' &#125; &#125;&#123; &lt;field&gt;: &#123; $regex: /pattern/&lt;options&gt; &#125; &#125; $options: i： 忽略大小写 m： For patterns that include anchors (i.e. ^ for the start, $ for the end),If the pattern contains no anchors or if the string value has no newline characters (e.g. \\n), the m option has no effect.（对于新的一行，m是不起作用的） x：“Extended” capability to ignore all white space characters in the $regexpattern unless escaped or included in a character class. Allows the dot character (i.e. .) to match all characters including newline characters. For an example, see Use the . Dot Character to Match New Line. example： 1&#123; name: &#123; $regex: /acme.*corp/, $options: \"si\" &#125; &#125;","text":"MongoDb 常用方法 $regx: 值匹配 123&#123; &lt;field&gt;: &#123; $regex: /pattern/, $options: '&lt;options&gt;' &#125; &#125;&#123; &lt;field&gt;: &#123; $regex: 'pattern', $options: '&lt;options&gt;' &#125; &#125;&#123; &lt;field&gt;: &#123; $regex: /pattern/&lt;options&gt; &#125; &#125; $options: i： 忽略大小写 m： For patterns that include anchors (i.e. ^ for the start, $ for the end),If the pattern contains no anchors or if the string value has no newline characters (e.g. \\n), the m option has no effect.（对于新的一行，m是不起作用的） x：“Extended” capability to ignore all white space characters in the $regexpattern unless escaped or included in a character class. Allows the dot character (i.e. .) to match all characters including newline characters. For an example, see Use the . Dot Character to Match New Line. example： 1&#123; name: &#123; $regex: /acme.*corp/, $options: \"si\" &#125; &#125; $unwind https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/ 1&#123; $unwind: &lt;field path&gt; &#125; 12345678&#123; $unwind: &#123; path: &lt;field path&gt;, includeArrayIndex: &lt;string&gt;, preserveNullAndEmptyArrays: &lt;boolean&gt; &#125;&#125; Field Type Description path string Field path to an array field. To specify a field path, prefix the field name with a dollar sign $ and enclose in quotes. 用$符号来获取属性中的数组字段 includeArrayIndex string Optional. The name of a new field to hold the array index of the element. The name cannot start with a dollar sign $. 输出在数组中的下标位置 preserveNullAndEmptyArrays boolean Optional. If true, if the path is null, missing, or an empty array,$unwind outputs the document. If false, $unwind does not output a document if the path is null, missing, or an empty array.The default value is false. true 代表输出，当数组是错误的，空的，或者空数组是，都输出来 Example 1&#123; \"_id\" : 1, \"item\" : \"ABC1\", sizes: [ \"S\", \"M\", \"L\"] &#125; # 原来文档 1db.inventory.aggregate( [ &#123; $unwind : \"$sizes\" &#125; ] ) # 执行语句 1234# 执行结果如下&#123; \"_id\" : 1, \"item\" : \"ABC1\", \"sizes\" : \"S\" &#125;&#123; \"_id\" : 1, \"item\" : \"ABC1\", \"sizes\" : \"M\" &#125; &#123; \"_id\" : 1, \"item\" : \"ABC1\", \"sizes\" : \"L\" &#125;","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"Mongo","slug":"Mongo","permalink":"https://jokinglove.com/blog/group/tags/Mongo/"},{"name":"Database","slug":"Database","permalink":"https://jokinglove.com/blog/group/tags/Database/"}]},{"title":"Mongodb 中两个操作 Reduce & Aggregate","slug":"others/database/20190619-mongodb-reduce&aggregate","date":"2019-05-15T16:00:00.000Z","updated":"2020-10-17T08:07:03.863Z","comments":true,"path":"2019/05/16/others/database/20190619-mongodb-reduce&aggregate/","link":"","permalink":"https://jokinglove.com/blog/2019/05/16/others/database/20190619-mongodb-reduce&aggregate/","excerpt":"Mongodb 中两个操作 Reduce &amp; Aggregate Map Reduce Map-Reduce 是一种计算模型，简单的说就是将大批量的工作(数据) 分解 (MAP) 执行，然后再将结果合并成最终的结果 (REDUCE)。 MongoDB 提供的 Map-Reduce 非常灵活，对于大规模数据分析也相当实用。","text":"Mongodb 中两个操作 Reduce &amp; Aggregate Map Reduce Map-Reduce 是一种计算模型，简单的说就是将大批量的工作(数据) 分解 (MAP) 执行，然后再将结果合并成最终的结果 (REDUCE)。 MongoDB 提供的 Map-Reduce 非常灵活，对于大规模数据分析也相当实用。 MapReduce 命令 以下是 MapReduce 的基本语法： 12345678910&gt; db.collection.mapReduce( funciton() &#123;emit(key, value);&#125;, // map 函数 funciton(key, value) &#123;return reduceFunction&#125;, // reduce 函数 &#123; out: collection, query: document, sort: document, limit: number &#125;) 实用 MapReduce 要实现两个函数 Map 函数和 Reduce 函数， Map 函数调用 emit(key, value)，遍历 collection 中所有的记录，将 key 与 value 传递给 Reduce 函数进行处理。 Map 函数必须调用 emit(key, value) 返回键值对。 参数说明： map: 映射函数 (生成键值对序列，作为 reduce 函数的参数)。 reduce： 统计函数，reduce 函数的任务就是将 key-value ，也就是把 values 数组变成一个单一的值 value。 out： 统计函数存放集合 (不指定则使用临时集合，在客户端断开后自动删除)。 query：一个筛选条件，只有满足条件的文档才会调用 map 函数。(query、limit、sort 可以随意组合) sort：和 limit 结合的 sort 排序菜蔬 (也是发往 map 函数前给文档排序)，可以优化分组机制。 limit：发往 map 函数的文档数量的上限 (要是没有 limit， 单独使用 sort 的用处不大)。 使用 MapReduce 考虑一下文档结构存储用户的文章，文档存储了用户的 user_name 和文章的 status 字段： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&gt;db.posts.insert(&#123; &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt;db.posts.insert(&#123; &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt;db.posts.insert(&#123; &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt;db.posts.insert(&#123; &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt;db.posts.insert(&#123; &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;disabled&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt;db.posts.insert(&#123; &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;disabled&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt;db.posts.insert(&#123; &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;disabled&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt;db.posts.insert(&#123; &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;active&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;) 现在，我们将在 posts 集合中使用 mapReduce 函数来选取已发布的文章 (status: “active”), 并通过 user_name 分组，计算每个用户的文章数： 12345678&gt; db.posts.mapReduce( function() &#123; emit(this.user_name, 1);&#125;, function(key, value) &#123;return Array.sum(values)&#125;, &#123; query: &#123;status: &quot;active&quot;&#125;, out: &quot;post_total&quot; &#125;) 以上 mapReduce 输出结果为： 1234567891011&#123; &quot;result&quot; : &quot;post_total&quot;, &quot;timeMillis&quot; : 23, &quot;counts&quot; : &#123; &quot;input&quot; : 5, &quot;emit&quot; : 5, &quot;reduce&quot; : 1, &quot;output&quot; : 2 &#125;, &quot;ok&quot; : 1&#125; 结果表明，共有 4 个符合查询条件 (status: “active”) 的文档，在 map 函数中生成了 4 个键值对文档，最后使用 reduce 函数将相同的键值分为两组。 具体参数说明： result：存储结果的 collection 的名字，这是个临时集合， MapReduce 的连接关闭后就自动被删除了。 timeMillis：执行花费的时间，毫秒为单位 input：满足条件被发送到 map 函数的文档个数。 emit：在 map 函数中 emit 被调动的次数，也就是所有集合找那个的数据总量。 output：结果结合中的文档个数 (count 对调试非常有帮助） ok： 是否成功，成功为 1。 err：如果失败，这里可以有失败原因，不过从经验来看，原因比较模糊，作用不大。 使用 find 操作符来查看 mapReduce 的查询结果： 12345678&gt;db.posts.mapReduce( function() &#123; emit(this.user_name,1); &#125;, function(key, values) &#123;return Array.sum(values)&#125;, &#123; query:&#123;status:&quot;active&quot;&#125;, out:&quot;post_total&quot; &#125;).find() 以上查询显示如下结果，两个用户 tom 和 mark 有两个发布的文章： 12&#123; &quot;_id&quot; : &quot;mark&quot;, &quot;value&quot; : 4 &#125;&#123; &quot;_id&quot; : &quot;runoob&quot;, &quot;value&quot; : 1 &#125; 用类似的方式，MapReduce可以被用来构建大型复杂的聚合查询。 Map函数和Reduce函数可以使用 JavaScript 来实现，使得MapReduce的使用非常灵活和强大。 Aggregate MongoDB 中聚合 (aggregate) 主要用于处理数据 (诸如统计平均值，求和等)，并返回计算后的数据结果。有点类似 sql 语句中的 count(*)。 aggregate() 方法 MongoDB 中聚合的方法使用 aggregate()。 语法 aggregate() 方法的基本语法格式如下所示： 1&gt;db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION) 实例 集合中的数据如下： 123456789101112131415161718192021222324252627&#123; _id: ObjectId(7df78ad8902c) title: 'MongoDB Overview', description: 'MongoDB is no sql database', by_user: 'w3cschool.cc', url: 'http://www.w3cschool.cc', tags: ['mongodb', 'database', 'NoSQL'], likes: 100 &#125;, &#123; _id: ObjectId(7df78ad8902d) title: 'NoSQL Overview', description: 'No sql database is very fast', by_user: 'w3cschool.cc', url: 'http://www.w3cschool.cc', tags: ['mongodb', 'database', 'NoSQL'], likes: 10 &#125;, &#123; _id: ObjectId(7df78ad8902e) title: 'Neo4j Overview', description: 'Neo4j is no sql database', by_user: 'Neo4j', url: 'http://www.neo4j.com', tags: ['neo4j', 'database', 'NoSQL'], likes: 750 &#125; 现在我们通过以上集合计算每个作者所写的文章数，使用 aggregate() 计算结果如下： 123456789101112131415&gt; db.mycol.aggregate([&#123;$group : &#123;_id : \"$by_user\", num_tutorial : &#123;$sum : 1&#125;&#125;&#125;]) &#123; \"result\" : [ &#123; \"_id\" : \"w3cschool.cc\", \"num_tutorial\" : 2 &#125;, &#123; \"_id\" : \"Neo4j\", \"num_tutorial\" : 1 &#125; ], \"ok\" : 1 &#125; &gt; 以上实例类似 sql 语句： select by_user, count(*) from mycol group by by_user; 在上面的例子中，我们通过字段 by_user 字段对数据进行分组，并计算 by_user 字段相同值得综合。 下标展示了一些集合的表达式： 表达式 描述 实例 $sum 计算总和。 db.mycol.aggregate([{$group : {_id : “$by_user”, num_tutorial : {$sum : “$likes”}}}]) $avg 计算平均值 db.mycol.aggregate([{$group : {_id : “$by_user”, num_tutorial : {$avg : “$likes”}}}]) $min 获取集合中所有文档对应值得最小值。 db.mycol.aggregate([{$group : {_id : “$by_user”, num_tutorial : {$min : “$likes”}}}]) $max 获取集合中所有文档对应值得最大值。 db.mycol.aggregate([{$group : {_id : “$by_user”, num_tutorial : {$max : “$likes”}}}]) $push 在结果文档中插入值到一个数组中。 db.mycol.aggregate([{$group : {_id : “$by_user”, url : {$push: “$url”}}}]) $addToSet 在结果文档中插入值到一个数组中，但不创建副本。 db.mycol.aggregate([{$group : {_id : “$by_user”, url : {$addToSet : “$url”}}}]) $first 根据资源文档的排序获取第一个文档数据。 db.mycol.aggregate([{$group : {_id : “$by_user”, first_url : {$first : “$url”}}}]) $last 根据资源文档的排序获取最后一个文档数据 db.mycol.aggregate([{$group : {_id : “$by_user”, last_url : {$last : “$url”}}}]) 管道的概念 管道在 Unix 和 Linux 中一般用于将当前命令的输出的结果作为下一个命令的参数。 MongoDB 的聚合管道将 MongoDB 文档在一个管道处理完毕后将结果传递给下一个管道处理。管道操作是可以重复的。 表达式：处理输入文档并输出。表达式是无状态的，只能用于计算当前聚合管道的文档，不能处理其他的文档。这里我们介绍一下聚合框架中常用的几个操作： $project: 修改输入文档的结构。可以用来重命名、增加或删除域，也可以用于创建计算结果以及嵌套文档。 $match: 用于过滤数据，只输出符合条件的文档。\\$match 使用 MongoDB 的标准查询操作。 $limit: 用来限制 MongoDB 聚合管道返回的文档数。 $skip：在聚合管道中跳过指定数量的文档，并返回余下的文档。 $unwind: 将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值。 $group: 将集合中的文档分组，可用于统计结果。 $sort: 将输入文档排序后输出。 $geoNear: 数据接近某一地理位置的有序文档。 管道操作符实例 $project实例 12345678db.article.aggregate( &#123; $project : &#123; title : 1 , author : 1 , &#125; &#125;); 这样的话结果中就只还有_id,tilte和author三个字段了，默认情况下_id字段是被包含的，如果要想不包含_id话可以这样: 123456789db.article.aggregate( &#123; $project : &#123; _id : 0 , title : 1 , author : 1 &#125; &#125;); $match实例 1234db.articles.aggregate( [ &#123; $match : &#123; score : &#123; $gt : 70, $lte : 90 &#125; &#125; &#125;, &#123; $group: &#123; _id: null, count: &#123; $sum: 1 &#125; &#125; &#125; ]); $skip实例 123db.article.aggregate( &#123; $skip : 5 &#125;); 经过$skip管道操作符处理后，前五个文档被&quot;过滤&quot;掉。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"Mongo","slug":"Mongo","permalink":"https://jokinglove.com/blog/group/tags/Mongo/"},{"name":"Database","slug":"Database","permalink":"https://jokinglove.com/blog/group/tags/Database/"}]},{"title":"markdown 常用语法汇总","slug":"others/tools/20190605-markdown常用语法","date":"2019-05-14T16:00:00.000Z","updated":"2020-10-17T08:07:03.869Z","comments":true,"path":"2019/05/15/others/tools/20190605-markdown常用语法/","link":"","permalink":"https://jokinglove.com/blog/2019/05/15/others/tools/20190605-markdown常用语法/","excerpt":"markdown 常用语法汇总 这些字有下划线 我有上标上标 我有下标下标 加粗1","text":"markdown 常用语法汇总 这些字有下划线 我有上标上标 我有下标下标 加粗1 加粗2 斜体1 斜体2 删除线 这个是定义变量","categories":[{"name":"上古神器","slug":"上古神器","permalink":"https://jokinglove.com/blog/group/categories/上古神器/"}],"tags":[{"name":"tools","slug":"tools","permalink":"https://jokinglove.com/blog/group/tags/tools/"}]},{"title":"Kafka (a distributed streaming platform)","slug":"others/kafka/20190524-spring-kafka","date":"2019-05-13T16:00:00.000Z","updated":"2020-10-17T08:07:03.866Z","comments":true,"path":"2019/05/14/others/kafka/20190524-spring-kafka/","link":"","permalink":"https://jokinglove.com/blog/2019/05/14/others/kafka/20190524-spring-kafka/","excerpt":"Kafka (a distributed streaming platform) Kafka 的三大功能： 发布 &amp; 订阅 类似于一个消息系统，读写流式的数据。 处理 编写可扩展的流处理应用程序，用于实时时间相应的场景。 存储 安全的将流式的数据存储在一个分布式、有副本备份、容错的集群。","text":"Kafka (a distributed streaming platform) Kafka 的三大功能： 发布 &amp; 订阅 类似于一个消息系统，读写流式的数据。 处理 编写可扩展的流处理应用程序，用于实时时间相应的场景。 存储 安全的将流式的数据存储在一个分布式、有副本备份、容错的集群。 Kafka 适合的场景： ​ Kafka 主要应用于两大类别的应用 1、构造实时流数据管道，它可以在系统或应用之间可靠的获取数据。(相当于 message queue) 2、构建实时流式应用程序，对这些流式数据进行转换或者影响。(就是流处理，通过 kafka strem topic 和 topic 之间内部进行变化) 一些概念 Kafka 作为一个集群，运行在一台或者多台服务器上。 Kafka 通过 topic 对存储的流数据进行分类。 每天记录包含一个 key ，一个 value 和一个 timestamp (时间戳)。 Kafka 的四个核心 API The Produce API 允许一个应用程序发布一串流式数据到一个或者多个 Kafka topic。 The Consumer API 允许一个应用程序订阅一个或者多个 topic，并对发布给他们的流式数据进行处理。 The Stream API 允许一个应用程序作为一个流处理器，消费一个或者多个 topic 产生的输入流，然后生产一个输出流到一个或者多个 topic 中去，在输入输出流中进行有效的转换。 The Connector API 允许构建并运行可重用的生产者或消费者，将 Kafka topic 连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表 (table) 的所有变更内容。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"Kafka","slug":"Kafka","permalink":"https://jokinglove.com/blog/group/tags/Kafka/"},{"name":"大数据","slug":"大数据","permalink":"https://jokinglove.com/blog/group/tags/大数据/"}]},{"title":"Spring Boot 整合 ElasticSearch 入门","slug":"others/microservice/20190524-spring-boot-elasticsearch","date":"2019-05-12T16:00:00.000Z","updated":"2020-10-17T08:07:03.868Z","comments":true,"path":"2019/05/13/others/microservice/20190524-spring-boot-elasticsearch/","link":"","permalink":"https://jokinglove.com/blog/2019/05/13/others/microservice/20190524-spring-boot-elasticsearch/","excerpt":"Spring Boot 整合 ElasticSearch 入门 ​ 利用 spring data elasticsearch 模块来对 elasticsearch 进行基础的增删改查，elasticsearch 利用 docker 的方式启动运行。 启动 elasticsearch 下载 elasticsearch 的 docker 镜像： 1docker pull docker.elastic.co/elasticsearch/elasticsearch:6.7.2 以开发模式运行 elasticsearch 的 docker 容器： 1docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.7.2","text":"Spring Boot 整合 ElasticSearch 入门 ​ 利用 spring data elasticsearch 模块来对 elasticsearch 进行基础的增删改查，elasticsearch 利用 docker 的方式启动运行。 启动 elasticsearch 下载 elasticsearch 的 docker 镜像： 1docker pull docker.elastic.co/elasticsearch/elasticsearch:6.7.2 以开发模式运行 elasticsearch 的 docker 容器： 1docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.7.2 浏览器访问 http://localhost:9200/ 出现如下信息就说明启动成功： 1234567891011121314151617&#123; name: \"-jrZGj7\", cluster_name: \"docker-cluster\", cluster_uuid: \"-3-MZwe_QImewxAtCSweKQ\", version: &#123; number: \"6.7.2\", build_flavor: \"default\", build_type: \"docker\", build_hash: \"56c6e48\", build_date: \"2019-04-29T09:05:50.290371Z\", build_snapshot: false, lucene_version: \"7.7.0\", minimum_wire_compatibility_version: \"5.6.0\", minimum_index_compatibility_version: \"5.0.0\" &#125;, tagline: \"You Know, for Search\"&#125; 编写测试程序来连接 elasticsearch 创建名为 spring-boot-elasticsearch 的 Spring Boot 项目，在 pom.xml 中添加下面的依赖： 12345678910111213&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- elasticsearch jar --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;&lt;/dependency&gt; 在 application.xml 文件中加入下面配置： 12345server.port= 8080spring.application.name = spring-boot-elasticsearch# 注意，这个位置一定要配置，不然会报 not avaliable node 的错误的，具体的 cluster-name 是从浏览器中访问 http://localhost:9200/ 接口中获取的spring.data.elasticsearch.cluster-name=docker-clusterspring.data.elasticsearch.cluster-nodes=127.0.0.1:9300 创建 GoodsIznfo.java 类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.ikang.springbootelasticsearch.document;import org.springframework.data.elasticsearch.annotations.Document;import java.io.Serializable;@Document(indexName = \"testgoods\", type = \"goods\")public class GoodsInfo implements Serializable &#123; private Long id; private String name; private String description; public GoodsInfo() &#123; &#125; public GoodsInfo(Long id, String name, String description) &#123; this.id = id; this.name = name; this.description = description; &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getDescription() &#123; return description; &#125; public void setDescription(String description) &#123; this.description = description; &#125;&#125; 创建 GoodsInfoRepository.java 接口继承 ElasticsearchRepository 接口 12345678910package com.ikang.springbootelasticsearch.repository;import com.ikang.springbootelasticsearch.document.GoodsInfo;import org.springframework.data.elasticsearch.repository.ElasticsearchRepository;import org.springframework.stereotype.Component;@Componentpublic interface GoodsRepository extends ElasticsearchRepository&lt;GoodsInfo, Long&gt; &#123;&#125; 创建 GoodsInfoController.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.ikang.springbootelasticsearch.controller;import com.ikang.springbootelasticsearch.document.GoodsInfo;import com.ikang.springbootelasticsearch.repository.GoodsRepository;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder;import org.springframework.data.domain.Page;import org.springframework.data.domain.PageRequest;import org.springframework.data.domain.Pageable;import org.springframework.data.elasticsearch.core.query.NativeSearchQuery;import org.springframework.data.elasticsearch.core.query.NativeSearchQueryBuilder;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.ArrayList;import java.util.List;import java.util.Optional;@RestController@RequestMapping(\"goods\")public class GoodsController &#123; private final GoodsRepository goodsRepository; public GoodsController(GoodsRepository goodsRepository) &#123; this.goodsRepository = goodsRepository; &#125; @GetMapping(\"save\") public String save() &#123; GoodsInfo goodsInfo = new GoodsInfo(System.currentTimeMillis(), \"商品\" + System.currentTimeMillis(), \"这是一个测试商品！\"); goodsRepository.save(goodsInfo); return \"success\"; &#125; @GetMapping(\"delete\") public String delete(Long id) &#123; goodsRepository.deleteById(id); return \"success\"; &#125; @GetMapping(\"update\") public String update(Long id, String name, String description) &#123; GoodsInfo goodsInfo = new GoodsInfo(id, name, description); goodsRepository.save(goodsInfo); return \"success\"; &#125; @GetMapping(\"getOne\") public GoodsInfo getOne(Long id) &#123; Optional&lt;GoodsInfo&gt; optional = goodsRepository.findById(id); return optional.isPresent() ? optional.get() : null; &#125; @GetMapping(\"list\") public List&lt;GoodsInfo&gt; list() &#123; Iterable&lt;GoodsInfo&gt; all = goodsRepository.findAll(); List&lt;GoodsInfo&gt; list = new ArrayList&lt;&gt;(); all.forEach(list::add); return list; &#125; // 每页查询数量 private final Integer DEFAULT_PAGE_SIZE = 10; @GetMapping(\"getList\") public Page&lt;GoodsInfo&gt; getList(Integer pageNum, String query) &#123; Pageable page = PageRequest.of(pageNum, DEFAULT_PAGE_SIZE); NativeSearchQuery nativeSearchQuery = new NativeSearchQueryBuilder() .withPageable(page) .withQuery(QueryBuilders.matchAllQuery()) .withFilter(QueryBuilders.matchQuery(\"name\", query)) .withFilter(QueryBuilders.matchQuery(\"description\", query)) .withHighlightFields(new HighlightBuilder.Field(\"name\").preTags(\"&lt;font color='red'&gt;\").postTags(\"&lt;/font&gt;\"), new HighlightBuilder.Field(\"description\").preTags(\"&lt;font color='red'&gt;\").postTags(\"&lt;/font&gt;\")) .build(); Page&lt;GoodsInfo&gt; goodsInfos = goodsRepository.search(nativeSearchQuery); return goodsInfos; &#125;&#125; 测试结果： 在浏览器中访问 http://localhost:8080/goods/save， http://localhost:8080/goods/update，http://localhost:8080/goods/list 将会看到所有数据已经插入，并且可以查询出来了： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&#123;content: [&#123;id: 1558662806732,name: \"商品1558662806732\",description: \"这是一个测试商品！\"&#125;,&#123;id: 1558662808245,name: \"商品1558662808245\",description: \"这是一个测试商品！\"&#125;,&#123;id: 1558662893476,name: \"商品1558662893476\",description: \"这是一个测试商品！\"&#125;,&#123;id: 1558662901290,name: \"商品1558662901290\",description: \"这是一个测试商品！\"&#125;,&#123;id: 1558662627656,name: \"商品1558662627656\",description: \"这是一个测试商品！\"&#125;,&#123;id: 1558662804196,name: \"商品1558662804196\",description: \"这是一个测试商品！\"&#125;,&#123;id: 1558662807792,name: \"商品1558662807792\",description: \"这是一个测试商品！\"&#125;,&#123;id: 1558662808433,name: \"商品1558662808433\",description: \"这是一个测试商品！\"&#125;,&#123;id: 1558662896876,name: \"商品1558662896876\",description: \"这是一个测试商品！\"&#125;,&#123;id: 1558662900326,name: \"商品1558662900326\",description: \"这是一个测试商品！\"&#125;],pageable: &#123;sort: &#123;unsorted: true,sorted: false,empty: true&#125;,offset: 10,pageSize: 10,pageNumber: 1,paged: true,unpaged: false&#125;,facets: [ ],aggregations: null,scrollId: null,maxScore: 1,totalElements: 26,totalPages: 3,size: 10,number: 1,sort: &#123;unsorted: true,sorted: false,empty: true&#125;,last: false,numberOfElements: 10,first: false,empty: false&#125; 遇到的问题： 1、启动报下面错误： 123456789101112132019-05-24 11:26:57.753 WARN 9683 --- [ main] o.e.c.t.TransportClientNodesService : node &#123;#transport#-1&#125;&#123;qZfhmDjMRiKldY_XKULvNg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125; not part of the cluster Cluster [elasticsearch], ignoring...2019-05-24 11:26:58.201 ERROR 9683 --- [ main] .d.e.r.s.AbstractElasticsearchRepository : failed to load elasticsearch nodes : org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: [&#123;#transport#-1&#125;&#123;qZfhmDjMRiKldY_XKULvNg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;]2019-05-24 11:26:58.350 INFO 9683 --- [ main] o.s.s.concurrent.ThreadPoolTaskExecutor : Initializing ExecutorService 'applicationTaskExecutor'2019-05-24 11:26:58.536 INFO 9683 --- [ main] o.s.b.a.e.web.EndpointLinksResolver : Exposing 2 endpoint(s) beneath base path '/actuator'2019-05-24 11:26:58.588 INFO 9683 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path ''2019-05-24 11:26:58.590 INFO 9683 --- [ main] c.i.s.SpringBootElasticsearchApplication : Started SpringBootElasticsearchApplication in 3.707 seconds (JVM running for 4.354)2019-05-24 11:26:58.648 INFO 9683 --- [)-172.16.98.240] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring DispatcherServlet 'dispatcherServlet'2019-05-24 11:26:58.648 INFO 9683 --- [)-172.16.98.240] o.s.web.servlet.DispatcherServlet : Initializing Servlet 'dispatcherServlet'2019-05-24 11:26:58.652 WARN 9683 --- [)-172.16.98.240] o.s.b.a.e.ElasticsearchHealthIndicator : Elasticsearch health check failedorg.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: [&#123;#transport#-1&#125;&#123;qZfhmDjMRiKldY_XKULvNg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;] at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:349) ~[elasticsearch-6.4.3.jar:6.4.3] at 问题分析： 这是因为 springboot 中默认的 cluster-name 为 elasticsearch，我们用 docker 启动的 elasticsearch 默认的名称是 docker-cluster, 所以在 application.xml 中加入以下配置就可以解决问题： 12&gt; spring.data.elasticsearch.cluster-name=docker-cluster&gt; 2、在查询过程中会报一个无法反序列化的问题，这个问题的解决方法是在 GoodsInfo.java 中加入一个默认的无参构造方法，具体为什么，应该和 jackson 的序列化有关。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"springboot","slug":"springboot","permalink":"https://jokinglove.com/blog/group/tags/springboot/"},{"name":"大数据","slug":"大数据","permalink":"https://jokinglove.com/blog/group/tags/大数据/"},{"name":"ES","slug":"ES","permalink":"https://jokinglove.com/blog/group/tags/ES/"},{"name":"搜索","slug":"搜索","permalink":"https://jokinglove.com/blog/group/tags/搜索/"}]},{"title":"美团系统监控工具 Cat 入门","slug":"others/microservice/20190512-cat","date":"2019-05-11T16:00:00.000Z","updated":"2020-10-17T08:07:03.868Z","comments":true,"path":"2019/05/12/others/microservice/20190512-cat/","link":"","permalink":"https://jokinglove.com/blog/2019/05/12/others/microservice/20190512-cat/","excerpt":"美团系统监控工具 Cat 入门 Cat 简介 CAT (Center Application Tracking) ，是基于 Java 开发的分布式实时监控系统。CAT 在基础存储、高性能童心、大规模在线访问、服务治理、实时监控、容器化及集群智能调度等领域提供业界领先的、统一的解决方案。CAT 目前在美团的产品定位是应用层的统一监控组件，基本接入了美团的所有核心应用，在中间件 (RPC、数据库、缓存、MQ等) 框架中得到广泛的应用，为各业务线提供系统的性能指标、健康状况、实时告警等。 Cat 的产品价值 减少线上问题的发现时间 减少问题故障的定位时间 辅助应用程序的优化工具","text":"美团系统监控工具 Cat 入门 Cat 简介 CAT (Center Application Tracking) ，是基于 Java 开发的分布式实时监控系统。CAT 在基础存储、高性能童心、大规模在线访问、服务治理、实时监控、容器化及集群智能调度等领域提供业界领先的、统一的解决方案。CAT 目前在美团的产品定位是应用层的统一监控组件，基本接入了美团的所有核心应用，在中间件 (RPC、数据库、缓存、MQ等) 框架中得到广泛的应用，为各业务线提供系统的性能指标、健康状况、实时告警等。 Cat 的产品价值 减少线上问题的发现时间 减少问题故障的定位时间 辅助应用程序的优化工具 Cat 的优势 实时处理：信息的价值会随时间锐减，尤其是事故处理过程中。 全量数据：最开始的设计目标就是全量采集，全量的好处有很多。 高可用：所有应用都倒下了，需要监控还站着，并告诉工程师发生了什么，做到故障还原和问题定位。 故障容忍：CAT 本身故障不应该影响业务正常运转，CAT 挂了，应用不应该受影响，只是监控能力暂时减弱。 高吞吐：要想还原真相，需要全方位地监控和度量，必须要有超强的处理吞吐能力。 可扩展：支持分布式、跨 IDC 部署，横向扩展的监控系统。 Cat 支持的监控消息类型： Transaction 适合记录跨越系统边界的程序访问行为，比如远程调用，数据库调用，也适合执行时间较长的业务逻辑监控，Transaction 用来记录一段代码的执行时间和次数。 Event 用来记录一件事发生的次数，比如记录系统异常，它和 transaction 相比缺少了时间的统计，开销比 transaction 要小。 Heartbeat 表示程序内定期产生的统计信息，如 CPU 利用率，内存利用率，连接池状态，系统负载等。 Metric 用于记录业务指标、指标可能包含对一个指标记录次数、记录平局值、记录总和，业务指标最低统计粒度为1分钟。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"监控","slug":"监控","permalink":"https://jokinglove.com/blog/group/tags/监控/"},{"name":"microservice","slug":"microservice","permalink":"https://jokinglove.com/blog/group/tags/microservice/"},{"name":"cat","slug":"cat","permalink":"https://jokinglove.com/blog/group/tags/cat/"}]},{"title":"Prometheus 入门","slug":"others/microservice/20190510-prometheus","date":"2019-05-10T16:00:00.000Z","updated":"2020-10-17T08:07:03.867Z","comments":true,"path":"2019/05/11/others/microservice/20190510-prometheus/","link":"","permalink":"https://jokinglove.com/blog/2019/05/11/others/microservice/20190510-prometheus/","excerpt":"Prometheus 入门 Prometheus 简介 Prometheus 是一个系统监控工具，支持。。。。 Prometheus 安装 通过连接中下载相应版本的 Prometheus Download 解压下载的文件 12tar xvfz prometheus-*.tar.gzcd prometheus-*","text":"Prometheus 入门 Prometheus 简介 Prometheus 是一个系统监控工具，支持。。。。 Prometheus 安装 通过连接中下载相应版本的 Prometheus Download 解压下载的文件 12tar xvfz prometheus-*.tar.gzcd prometheus-* Prometheus 的使用 ​ 这部分主要用 Prometheus 来监控自己的使用： 创建文件 prometheus.yml，内容如下： 12345678910111213141516171819global: scrape_interval: 15s # By default, scrape targets every 15 seconds. # Attach these labels to any time series or alerts when communicating with # external systems (federation, remote storage, Alertmanager). external_labels: monitor: 'codelab-monitor'# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9090'] 启动 Prometheus 123# Start Prometheus.# By default, Prometheus stores its database in ./data (flag --storage.tsdb.path)../prometheus --config.file=prometheus.yml 默认的，Prometheus 开启了 9090 端口，我们可以通过访问 http://localhost:9090 来查看 Prometheus 中的监控信息。 总结","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"promethus","slug":"promethus","permalink":"https://jokinglove.com/blog/group/tags/promethus/"},{"name":"监控","slug":"监控","permalink":"https://jokinglove.com/blog/group/tags/监控/"},{"name":"microservice","slug":"microservice","permalink":"https://jokinglove.com/blog/group/tags/microservice/"}]},{"title":"spring容器初始化的几种方式","slug":"others/java/spring-容器初始化的几种方式","date":"2019-04-24T16:00:00.000Z","updated":"2020-10-17T08:07:03.866Z","comments":true,"path":"2019/04/25/others/java/spring-容器初始化的几种方式/","link":"","permalink":"https://jokinglove.com/blog/2019/04/25/others/java/spring-容器初始化的几种方式/","excerpt":"spring 通常有五种初始化的方式，代码如下：","text":"spring 通常有五种初始化的方式，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package ssh.spring; import java.io.IOException; import org.springframework.beans.factory.BeanFactory;import org.springframework.beans.factory.xml.XmlBeanFactory;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.core.io.ClassPathResource;import org.springframework.core.io.FileSystemResource;import org.springframework.core.io.Resource; public class Test &#123; @org.junit.Test public void test1()&#123; ApplicationContext ac=new ClassPathXmlApplicationContext(\"ssh/spring/applicationContext.xml\"); Person p1=(Person)ac.getBean(\"person\"); System.out.println(\"test1 \"+p1); &#125; @org.junit.Test public void test2()&#123; ApplicationContext ac=new ClassPathXmlApplicationContext(\"applicationContext.xml\",this.getClass()); Person p1=(Person)ac.getBean(\"person\"); System.out.println(\"test2 \"+p1); &#125; @org.junit.Test public void test3()&#123; Resource resource=new ClassPathResource(\"ssh/spring/applicationContext.xml\"); BeanFactory beanFactory=new XmlBeanFactory(resource); Person p1=(Person)beanFactory.getBean(\"person\"); System.out.println(\"test3 \"+p1); &#125; @org.junit.Test public void test4() throws IOException&#123; Resource resource=new ClassPathResource(\"applicationContext.xml\",this.getClass()); BeanFactory beanFactory=new XmlBeanFactory(resource); Person p1=(Person)beanFactory.getBean(\"person\"); System.out.println(\"test4 \"+p1); &#125; @org.junit.Test public void test5()&#123; Resource resource=new FileSystemResource(\"E:/Java/study/WebRoot/WEB-INF/classes/ssh/spring/applicationContext.xml\"); BeanFactory beanFactory=new XmlBeanFactory(resource); Person p1=(Person)beanFactory.getBean(\"person\"); System.out.println(\"test5 \"+p1); &#125;&#125;","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://jokinglove.com/blog/group/tags/spring/"}]},{"title":"python 学习笔记1","slug":"others/python1","date":"2018-07-21T16:00:00.000Z","updated":"2020-10-17T08:07:03.868Z","comments":true,"path":"2018/07/22/others/python1/","link":"","permalink":"https://jokinglove.com/blog/2018/07/22/others/python1/","excerpt":"Python3中有六个标准的数据类型： 字符串（String） 数字（Digit） 列表（List）","text":"Python3中有六个标准的数据类型： 字符串（String） 数字（Digit） 列表（List） 元组（Tuple） 集合（Sets） 字典（Dictionary） 日期（date） 布尔 (Boolean) 数值 (Number) 整型——int——数字 python有5种数字类型，最常见的就是整型int。例如：1234、-1234 布尔型——bool——用符号==表示 布尔型是一种比较特殊的python数字类型，它只有True和False两种值，它主要用来比较和判断，所得结果叫做布尔值。例如：3==3 给出True，3==5给出False 字符串——str——用’ '或&quot; “表示 例如：'www.iplaypython.com’或者&quot;hello” 列表——list——用[ ]符号表示 例如：[1,2,3,4] 元组——tuple——用( )符号表示 例如：（‘d’,300） 字典——dict——用{ }符号表示 例如：｛‘name’:‘coco’,‘country’:‘china’｝ 更多内容…","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"python","slug":"python","permalink":"https://jokinglove.com/blog/group/tags/python/"}]},{"title":"maven install","slug":"others/maveninstall","date":"2017-12-06T16:00:00.000Z","updated":"2020-10-17T08:07:03.867Z","comments":true,"path":"2017/12/07/others/maveninstall/","link":"","permalink":"https://jokinglove.com/blog/2017/12/07/others/maveninstall/","excerpt":"使用maven install安装jar包到本地仓库 我们经常用遇到一个问题，那就是每次在项目更新的时候，maven 就一直卡在那不动了，等半天都等不住，尤其是在eclipse中，有时候啥也做不了就一直等着。打开进程发现，原来在下jar包的时候，一直在找，但就是下载不下来，有些就1kb，2kb的在那下载这，看着特别蛋疼。这时，我们就可以去maven中心仓库或者其他地方手动下载jar包，然后通过maven install命令，将我们下载好的jar包直接安装到本地仓库中。","text":"使用maven install安装jar包到本地仓库 我们经常用遇到一个问题，那就是每次在项目更新的时候，maven 就一直卡在那不动了，等半天都等不住，尤其是在eclipse中，有时候啥也做不了就一直等着。打开进程发现，原来在下jar包的时候，一直在找，但就是下载不下来，有些就1kb，2kb的在那下载这，看着特别蛋疼。这时，我们就可以去maven中心仓库或者其他地方手动下载jar包，然后通过maven install命令，将我们下载好的jar包直接安装到本地仓库中。 1、将Jar包安装到本地仓库 命令 1mvn install:install-file -Dfile=D:\\thrift-0.9.2.jar -DgroupId=org.apache.thrift -DartifactId=libthrift -Dversion=0.9.2 -Dpackaging=jar 其中： DgroupId和DartifactId构成了该jar包在pom.xml的坐标， 对应依赖的DgroupId和DartifactId Dfile表示需要上传的jar包的绝对路径 Dpackaging 为安装文件的种类 2、 上传Jar到私服 命令 1mvn deploy:deploy-file -DgroupId=org.apache.thrift -DartifactId=libthrift -Dversion=1.12 -Dpackaging=jar -Dfile=D:\\thrift-0.9.2.jar -Durl=http://ip:port/nexus/content/repositories/thirdparty/ -DrepositoryId=thirdparty 其中： DgroupId和DartifactId构成了该jar包在pom.xml的坐标， 对应依赖的DgroupId和DartifactId Dfile表示需要上传的jar包的绝对路径 Durl私服上仓库的url精确地址(打开nexus左侧repositories菜单，可以看到该路径) DrepositoryId服务器的表示id，在nexus的configuration可以看到 这样我们就可以直接在我们的pom.xml文件中直接导入dependecy了，不需要去网上下载了，不会因为某几个jar包的问题，而耽误做其他的事了。","categories":[{"name":"上古神器","slug":"上古神器","permalink":"https://jokinglove.com/blog/group/categories/上古神器/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://jokinglove.com/blog/group/tags/maven/"}]},{"title":"maven install","slug":"others/tools/maveninstall","date":"2017-12-06T16:00:00.000Z","updated":"2020-10-17T08:07:03.869Z","comments":true,"path":"2017/12/07/others/tools/maveninstall/","link":"","permalink":"https://jokinglove.com/blog/2017/12/07/others/tools/maveninstall/","excerpt":"使用maven install安装jar包到本地仓库 我们经常用遇到一个问题，那就是每次在项目更新的时候，maven 就一直卡在那不动了，等半天都等不住，尤其是在eclipse中，有时候啥也做不了就一直等着。打开进程发现，原来在下jar包的时候，一直在找，但就是下载不下来，有些就1kb，2kb的在那下载这，看着特别蛋疼。这时，我们就可以去maven中心仓库或者其他地方手动下载jar包，然后通过maven install命令，将我们下载好的jar包直接安装到本地仓库中。","text":"使用maven install安装jar包到本地仓库 我们经常用遇到一个问题，那就是每次在项目更新的时候，maven 就一直卡在那不动了，等半天都等不住，尤其是在eclipse中，有时候啥也做不了就一直等着。打开进程发现，原来在下jar包的时候，一直在找，但就是下载不下来，有些就1kb，2kb的在那下载这，看着特别蛋疼。这时，我们就可以去maven中心仓库或者其他地方手动下载jar包，然后通过maven install命令，将我们下载好的jar包直接安装到本地仓库中。 1、将Jar包安装到本地仓库 命令 1mvn install:install-file -Dfile=D:\\thrift-0.9.2.jar -DgroupId=org.apache.thrift -DartifactId=libthrift -Dversion=0.9.2 -Dpackaging=jar 其中： DgroupId和DartifactId构成了该jar包在pom.xml的坐标， 对应依赖的DgroupId和DartifactId Dfile表示需要上传的jar包的绝对路径 Dpackaging 为安装文件的种类 2、 上传Jar到私服 命令 1mvn deploy:deploy-file -DgroupId=org.apache.thrift -DartifactId=libthrift -Dversion=1.12 -Dpackaging=jar -Dfile=D:\\thrift-0.9.2.jar -Durl=http://ip:port/nexus/content/repositories/thirdparty/ -DrepositoryId=thirdparty 其中： DgroupId和DartifactId构成了该jar包在pom.xml的坐标， 对应依赖的DgroupId和DartifactId Dfile表示需要上传的jar包的绝对路径 Durl私服上仓库的url精确地址(打开nexus左侧repositories菜单，可以看到该路径) DrepositoryId服务器的表示id，在nexus的configuration可以看到 这样我们就可以直接在我们的pom.xml文件中直接导入dependecy了，不需要去网上下载了，不会因为某几个jar包的问题，而耽误做其他的事了。","categories":[{"name":"上古神器","slug":"上古神器","permalink":"https://jokinglove.com/blog/group/categories/上古神器/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://jokinglove.com/blog/group/tags/maven/"}]},{"title":"从此以后，gitpages将不再受主题限制","slug":"others/tools/git-pages-themes","date":"2017-11-29T16:00:00.000Z","updated":"2020-10-17T08:07:03.869Z","comments":true,"path":"2017/11/30/others/tools/git-pages-themes/","link":"","permalink":"https://jokinglove.com/blog/2017/11/30/others/tools/git-pages-themes/","excerpt":"今天上github，突然发现了一个令人愉快的事，写个博客庆祝一波，那就是git pages 功能支持任何 public的git themes 了。赶紧把自己的一个工具给换了个主题，瞬间感觉高大上了，😄 小伙伴们可以参考一下","text":"今天上github，突然发现了一个令人愉快的事，写个博客庆祝一波，那就是git pages 功能支持任何 public的git themes 了。赶紧把自己的一个工具给换了个主题，瞬间感觉高大上了，😄 小伙伴们可以参考一下 更换theme很方便，下面是官方消息的截图： 更换themes灰常简单，在自己要作为gitpages 的项目更目录新建一个_config.yml的文件，在里面加上这一句就ok了，然后git在构建的时候，会自动去相应的仓库下面拿到主题，应用的你的git pages 上面： 1remote_theme: owner/name 其中 owner是主体所有者的名称，name 就是对应的主题了。就像这样： 1remote_theme: pages-themes/leap-day 小伙伴们可以赶紧给自己的git pages换个高大上的主题玩玩了，有好的主题，也分享一波，我也换一个，一直是一个选择困难症患者，不知道什么样的主题比较适合我。但是不可否认的是，git，github很屌，屌屌的。💯","categories":[{"name":"上古神器","slug":"上古神器","permalink":"https://jokinglove.com/blog/group/categories/上古神器/"}],"tags":[{"name":"git","slug":"git","permalink":"https://jokinglove.com/blog/group/tags/git/"}]},{"title":"节日感想","slug":"others/life/20171024-1024节日感想","date":"2017-10-23T16:00:00.000Z","updated":"2020-10-17T08:07:03.867Z","comments":true,"path":"2017/10/24/others/life/20171024-1024节日感想/","link":"","permalink":"https://jokinglove.com/blog/2017/10/24/others/life/20171024-1024节日感想/","excerpt":"好久没有写过博客了，今天是个特殊的日子。1024，写一篇博客扯扯蛋。不知道1024的程序员一定是个假程序员。 本来应该在这么个特殊的日子里面写一篇技术文章应该感觉会好一些，但是今天的心情貌似只适合扯淡，所以就不写技术相关的了，专门扯淡。","text":"好久没有写过博客了，今天是个特殊的日子。1024，写一篇博客扯扯蛋。不知道1024的程序员一定是个假程序员。 本来应该在这么个特殊的日子里面写一篇技术文章应该感觉会好一些，但是今天的心情貌似只适合扯淡，所以就不写技术相关的了，专门扯淡。 最近正在看一本叫做《新生》的书，有一些感触，那就扯扯从这本书中看到的一些感悟吧。这本书是李笑来老师写的，作为一个全面发展的人（至少我认为是这样的一个人），书中将人比作电脑，将我们所说的三观比作操作系统。每个人都有一个操作系统，而有些人会定期的升级操作系统，让自己的操作系统更加的灵活好用，而有些人的操作系统已经好久没有升级过了，还处于dos时代，有些人甚至没有操作系统。细细想来还真有点那么个意思。 对于大多数人来说，生活在北京这么个压力大，生活节奏快的城市里，用我自己的话说就是能活下来已经不错了，先活下来，然后再想怎么去活的更好，每天在不知所以的奋斗着。也不知道什么时候会更好，但是只是一味的奋斗貌似并不能更快的让自己活的更好，让自己的操作系统升级的更强。这个时候就需要我们去升级我们的操作系统，升级操作系统，有些人靠着自己的天赋，可以发现自己的缺点，不足，主动去给自己的操作系统打个补丁，然而大部分人并不是那一类人。而是需要去学习别人的有点，别人的方法论，学习新的概念。来升级自己的操作系统。这就像是在提取需求一样，你看完一本书，感觉对的，对自己有用的，认为是正确的方法论，就可以提取出来，作为你升级操作系统的用例。 电脑是一些极其聪明的人研究出来的，那一部分人也就是需要我们去学习的对象，他们将电脑直接连接的方式不是写死了就这么做，而是定义成了接口，可以通过统一的接口和不同的电脑、软件、或者硬件向连接。而我们的沟通能力，就像是一个网卡和无线连接一样，有些人只能连接到局域网，有些人可以连接到城域网，还有一些人就可以在互联网上畅通无阻的连接。审视一下自己，发现自己也就只能连接到局域网，这貌似和我的期望有所出入的，就想是不是我的操作系统出了问题。是不是应该升级一波操作系统了，升级的重点就是看书了，从书中找到自己的bug，主动去打个补丁，恩，听起来是个不错的选择。 升级自己的操作系统的另外一条途径就是和学习身边的人，学习身边人的优点，好的方法论，处事方式，给自己打个补丁。升级自己的操作系统，先从看书开始，先从升级自己的操作系统开始吧。就这样，开始干吧。","categories":[{"name":"瞎写","slug":"瞎写","permalink":"https://jokinglove.com/blog/group/categories/瞎写/"}],"tags":[{"name":"1024","slug":"1024","permalink":"https://jokinglove.com/blog/group/tags/1024/"}]},{"title":"spring helloworld","slug":"others/spring-boot-helloworld","date":"2017-09-09T14:00:58.000Z","updated":"2020-10-17T08:07:03.869Z","comments":true,"path":"2017/09/09/others/spring-boot-helloworld/","link":"","permalink":"https://jokinglove.com/blog/2017/09/09/others/spring-boot-helloworld/","excerpt":"忙了这么久，感觉好久都没有学习一点新技术了，之前看过spring-boot的一些文档，现在打算实践一遍。很强势的，我们从hello world 开始。从不用ide开始。","text":"忙了这么久，感觉好久都没有学习一点新技术了，之前看过spring-boot的一些文档，现在打算实践一遍。很强势的，我们从hello world 开始。从不用ide开始。 1、新建一个maven项目。 可以用以下命令 12mkdir -p spring-boot-hello/src/main/javatouch pom.xml 其中pom.xml文件中输入以下内容 123456789101112131415161718192021222324&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-hello&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;!-- 这个代表着此项目是从 parent中继承来的 --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.M3&lt;/version&gt; &lt;/parent&gt; &lt;!-- 添加web相关的包支持 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2、然后创建一个名为HelloController.java的文件 123mkdir -p com/hellocd com/hello/touch HelloController.java 其中内容如下: 1234567891011package com.hello;@RestController@RequestMapping(\"/hello\")public class HelloController&#123; @RequestMapping(\"/say\") public String sayHello()&#123; return \"Hello World!\"; &#125;&#125; 3、返回到java目录，新建名为Application.java的文件，内容如下: 123456@EnableAutoConfigurationpublic class Application &#123; public static void main(String[] args) throws Exception&#123; SpringApplication.run(Application.class, args); &#125;&#125; 4、利用mvn package 打包，进入到target目录下,生成一个以 spring-boot-hello 开头的jar文件。 5、用 java -jar 命令运行这个jar文件。可以在命令行看到spring-boot的启动信息。 6、在浏览器中用 localhost:8080/hello/say 访问，就可以看到 Hello World! 了。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"spring-boot","slug":"spring-boot","permalink":"https://jokinglove.com/blog/group/tags/spring-boot/"}]},{"title":"魔鬼在细节中","slug":"others/魔鬼在细节中","date":"2017-08-08T16:00:00.000Z","updated":"2020-10-17T08:07:03.870Z","comments":true,"path":"2017/08/09/others/魔鬼在细节中/","link":"","permalink":"https://jokinglove.com/blog/2017/08/09/others/魔鬼在细节中/","excerpt":"作为一个it小司机，每次在写完代码，自己测了一遍都没发现什么问题，提交给测试的时候反馈回来一大堆的bug，检查发现很多其实都是一些很小的需要注意的细节，但却足以让你的程序瘫痪，平时注意这些小细节，可能会省去好多事。那么在编码中我们应该时候需要注意的规范有哪些呢，这也是在成为一个it老司机的路上必须要经过的一道坎，在这借鉴一下阿里大牛写的一遍魔鬼在细节中。","text":"作为一个it小司机，每次在写完代码，自己测了一遍都没发现什么问题，提交给测试的时候反馈回来一大堆的bug，检查发现很多其实都是一些很小的需要注意的细节，但却足以让你的程序瘫痪，平时注意这些小细节，可能会省去好多事。那么在编码中我们应该时候需要注意的规范有哪些呢，这也是在成为一个it老司机的路上必须要经过的一道坎，在这借鉴一下阿里大牛写的一遍魔鬼在细节中。 防止空指针和下标越界 这是我最不喜欢看到的异常，尤其在核心框架中，我更愿看到信息详细的参数不合法异常， 这也是一个健状的程序开发人员，在写每一行代码都应在潜意识中防止的异常， 基本上要能确保一次写完的代码，在不测试的情况，都不会出现这两个异常才算合格。 保证线程安全性和可见性 对于框架的开发人员，对线程安全性和可见性的深入理解是最基本的要求， 需要开发人员，在写每一行代码时都应在潜意识中确保其正确性， 因为这种代码，在小并发下做功能测试时，会显得很正常， 但在高并发下就会出现莫明其妙的问题，而且场景很难重现，极难排查。 尽早失败和前置断言 尽早失败也应该成为潜意识，在有传入参数和状态变化时，均在入口处全部断言， 一个不合法的值和状态，在第一时间就应报错，而不是等到要用时才报错， 因为等到要用时，可能前面已经修改其它相关状态，而在程序中很少有人去处理回滚逻辑， 这样报错后，其实内部状态可能已经混乱，极易在一个隐蔽分支上引发程序不可恢复。 分离可靠操作和不可靠操作 这里的可靠是狭义的指是否会抛出异常或引起状态不一致， 比如，写入一个线程安全的Map，可以认为是可靠的， 而写入数据库等，可以认为是不可靠的， 开发人员必须在写每一行代码时，都注意它的可靠性与否， 在代码中尽量划分开，并对失败做异常处理， 并为容错，自我保护，自动恢复或切换等补偿逻辑提供清晰的切入点， 保证后续增加的代码不至于放错位置，而导致原先的容错处理陷入混乱。 异常防御，但不忽略异常 这里讲的异常防御，指的是对非必须途径上的代码进行最大限度的容忍， 包括程序上的BUG，比如：获取程序的版本号，会通过扫描Manifest和jar包名称抓取版本号， 这个逻辑是辅助性的，但代码却不少，初步测试也没啥问题， 但应该在整个getVersion()中加上一个全函数的try-catch打印错误日志，并返回基本版本， 因为getVersion()可能存在未知特定场景异常，或被其他的开发人员误修改逻辑(但一般人员不会去掉try-catch)， 而如果它抛出异常会导致主流程异常，这是我们不希望看到的， 但这里要控制个度，不要随意try-catch，更不要无声无息的吃掉异常。 缩小可变域和尽量final 如果一个类可以成为不变类(Immutable Class)，就优先将它设计成不变类， 不变类有天然的并发共享优势，减少同步或复制，而且可以有效帮忙分析线程安全的范围， 就算是可变类，对于从构造函数传入的引用，在类中持有时，最好将字段final，以免被中途误修改引用， 不要以为这个字段是私有的，这个类的代码都是我自己写的，不会出现对这个字段的重新赋值， 要考虑的一个因素是，这个代码可能被其他人修改，他不知道你的这个弱约定，final就是一个不变契约。 降低修改时的误解性，不埋雷 前面不停的提到代码被其他人修改，这也开发人员要随时紧记的， 这个其他人包括未来的自己，你要总想着这个代码可能会有人去改它， 我应该给修改的人一点什么提示，让他知道我现在的设计意图， 而不要在程序里面加潜规则，或埋一些容易忽视的雷， 比如：你用null表示不可用，size等于0表示黑名单， 这就是一个雷，下一个修改者，包括你自己，都不会记得有这样的约定， 可能后面为了改某个其它BUG，不小心改到了这里，直接引爆故障。 对于这个例子，一个原则就是永远不要区分null引用和empty值。 提高代码的可测性 这里的可测性主要指Mock的容易程度，和测试的隔离性， 至于测试的自动性，可重复性，非偶然性，无序性，完备性(全覆盖)，轻量性(可快速执行)， 一般开发人员，加上JUnit等工具的辅助基本都能做到，也能理解它的好处，只是工作量问题， 这里要特别强调的是测试用例的单一性(只测目标类本身)和隔离性(不传染失败)， 现在的测试代码，过于强调完备性，大量重复交叉测试， 看起来没啥坏处，但测试代码越多，维护代价越高， 经常出现的问题是，修改一行代码或加一个判断条件，引起100多个测试用例不通过， 时间一紧，谁有这个闲功夫去改这么多形态各异的测试用例？ 久而久之，这个测试代码就已经不能真实反应代码现在的状况，很多时候会被迫绕过， 最好的情况是，修改一行代码，有且只有一行测试代码不通过， 如果修改了代码而测试用例还能通过，那也不行，表示测试没有覆盖到， 另外，可Mock性是隔离的基础，把间接依赖的逻辑屏蔽掉， 可Mock性的一个最大的杀手就是静态方法，尽量少用。 原文地址…","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"编码规范","slug":"编码规范","permalink":"https://jokinglove.com/blog/group/tags/编码规范/"}]},{"title":"类与类之间的关系","slug":"others/class-relationship","date":"2017-07-31T16:00:00.000Z","updated":"2020-10-17T08:07:03.863Z","comments":true,"path":"2017/08/01/others/class-relationship/","link":"","permalink":"https://jokinglove.com/blog/2017/08/01/others/class-relationship/","excerpt":"在面向对象分析的过程中，我们常常会首先分析类与类之间有什么关系，然后设计出合理的类结构，尽量达到低耦合，高内聚。使得程序的可扩展性更高，下面简单的总结一下在面向对象的世界里类与类之间的关系。","text":"在面向对象分析的过程中，我们常常会首先分析类与类之间有什么关系，然后设计出合理的类结构，尽量达到低耦合，高内聚。使得程序的可扩展性更高，下面简单的总结一下在面向对象的世界里类与类之间的关系。 1、泛化关系(generalization) 类的继承结构表现在UML中为：泛化(generalize)与实现(realize)： 继承关系为 is-a的关系；两个对象之间如果可以用 is-a 来表示，就是继承关系：（…是…) eg：自行车是车、猫是动物 泛化关系用一条带空心箭头的直接表示； 注：最终代码中，泛化关系表现为继承非抽象类； 2、实现关系(realize) 实现关系用一条带空心箭头的虚线表示； eg：”车”为一个抽象概念，在现实中并无法直接用来定义对象；只有指明具体的子类(汽车还是自行车)，才 可以用来定义对象（”车”这个类在C++中用抽象类表示，在JAVA中有接口这个概念，更容易理解） 注：最终代码中，实现关系表现为继承抽象类； 3、聚合关系(aggregation) 聚合关系用一条带空心菱形箭头的直线表示，如下图表示A聚合到B上，或者说B由A组成； 聚合关系用于表示实体对象之间的关系，表示整体由部分构成的语义；例如一个部门由多个员工组成； 与组合关系不同的是，整体和部分不是强依赖的，即使整体不存在了，部分仍然存在；例如， 部门撤销了，人员不会消失，他们依然存在； 4、组合关系(composition) 组合关系用一条带实心菱形箭头直线表示，如下图表示A组成B，或者B由A组成； 与聚合关系一样，组合关系同样表示整体由部分构成的语义；比如公司由多个部门组成； 但组合关系是一种强依赖的特殊聚合关系，如果整体不存在了，则部分也不存在了；例如， 公司不存在了，部门也将不存在了； 5、关联关系(association) 关联关系是用一条直线表示的；它描述不同类的对象之间的结构关系；它是一种静态关系， 通常与运行状态无关，一般由常识等因素决定的；它一般用来定义对象之间静态的、天然的结构； 所以，关联关系是一种“强关联”的关系； 比如，乘车人和车票之间就是一种关联关系；学生和学校就是一种关联关系； 关联关系默认不强调方向，表示对象间相互知道；如果特别强调方向，如下图，表示A知道B，但 B不知道A； 注：在最终代码中，关联对象通常是以成员变量的形式实现的； 6、依赖关系(dependency) 依赖关系是用一套带箭头的虚线表示的；如下图表示A依赖于B；他描述一个对象在运行期间会用到另一个对象的关系； 与关联关系不同的是，它是一种临时性的关系，通常在运行期间产生，并且随着运行时的变化； 依赖关系也可能发生变化； 显然，依赖也有方向，双向依赖是一种非常糟糕的结构，我们总是应该保持单向依赖，杜绝双向依赖的产生； 注：在最终代码中，依赖关系体现为类构造方法及类方法的传入参数，箭头的指向为调用关系；依赖关系除了临时知道对方外，还是“使用”对方的方法和属性；","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"面向对象","slug":"面向对象","permalink":"https://jokinglove.com/blog/group/tags/面向对象/"}]},{"title":"七夕瞎写","slug":"others/life/20170707-七夕瞎写","date":"2017-07-06T16:00:00.000Z","updated":"2020-10-17T08:07:03.866Z","comments":true,"path":"2017/07/07/others/life/20170707-七夕瞎写/","link":"","permalink":"https://jokinglove.com/blog/2017/07/07/others/life/20170707-七夕瞎写/","excerpt":"听说今天是七夕，情人节，首先祝天下有情人终成眷属。 七夕快乐","text":"听说今天是七夕，情人节，首先祝天下有情人终成眷属。 七夕快乐 关于这个七夕节 以前，每次过七夕也没怎么有什么问题，可今年的七夕，总感觉乖乖的。以往的七夕都是看着人家过，今年总感觉自己也在过，虽然这个七夕不重要，但是，七夕也得有个七夕的计划，对于我这个资深单身狗的计划。 关于工作 像我这种单身狗，感觉除了工作，也没什么可以写的了。最近工作有点忙，也没什么时间来看技术文档，没有时间研究一点新技术了。都说一入it深似海，从此妹子是路人，对于爱情这个问题，我一向所持的态度是，有可能的可以试试，没可能的，敬而远之。这就像一朵开的很艳丽的鲜花，既然你不能得到它，就让他鲜艳的开着，何必去摘呢。爱情这件事，更需要的是勇气，可能我还是比较缺乏勇气的。很乱。 对于一个家庭来说，稳定的收入，和睦的成员关系。最重要的是两个人的相濡以沫，粗茶淡饭的天长地久，这些能不能经受得住时间的考验。对于一个家庭来说，我觉得很重要。然而，对于一个男人的事业来说，直接影响这这个家庭的一切，这很疼，不说了，我还是加油敲代码去了，不敲代码，哪来的钱，没有钱，哪来的媳妇，没有媳妇，哪来的家庭，光在这儿吹逼是吹不出来对的。敲代码，敲代码。你也别看了，赶紧敲代码去。。。。。","categories":[{"name":"瞎写","slug":"瞎写","permalink":"https://jokinglove.com/blog/group/categories/瞎写/"}],"tags":[{"name":"随记","slug":"随记","permalink":"https://jokinglove.com/blog/group/tags/随记/"}]},{"title":"git 笔记 fork","slug":"others/tools/git-note1","date":"2017-06-17T14:00:58.000Z","updated":"2020-10-17T08:07:03.869Z","comments":true,"path":"2017/06/17/others/tools/git-note1/","link":"","permalink":"https://jokinglove.com/blog/2017/06/17/others/tools/git-note1/","excerpt":"今天看到一个博客中对git几个命令的总结，感觉讲的通俗易懂，借过来记一笔","text":"今天看到一个博客中对git几个命令的总结，感觉讲的通俗易懂，借过来记一笔 需求：B要加入A的项目，不论是作为B的初始项目进行二次开发还是成为A项目的一员加入一起开发，步骤如下： 1.B首先要fork一个。 B首先到A的github上，也就是此项目的位置:https://github.com/A/durit，然后单击fork，然后你（B）的github上就出现了一个fork，位置是：https://github.com/B/durit 2.B把自己的fork克隆到本地。 1$ git clone https://github.com/B/durit (当你clone到本地，会有一个默认的远程名叫&quot;origin&quot;，它指向了fork on github，也就是B上的fork，而不是指向) 3.现在你是主人，为了保持与A的durit的联系，你需要给A的durit起个名，供你来驱使。 12$ cd durit$ git remote add upstream https://github.com/A/durit (现在改名为upstream，这名随意，现在你（B）管A的durit叫upstream，以后B就用upstream来和A的durit联系了) 4.获取A上的更新(但不会修改你的文件)。 1$ git fetch upstream （这不，现在B就用upstream来联系A了） 5.合并拉取的数据 1$ git merge upstream/master （又联系了一次，upstream/master，前者是你要合并的数据，后者是你要合并到的数据（在这里就是B本地的durit了）） 6.在B修改了本地部分内容后，把本地的更改推送到B的远程github上。 123$ git add 修改过的文件$ git commit -m \"注释\"$ git push origin master （目前为止，B上的github就跟新了） 7.然后B还想让修改过的内容也推送到A上，这就要发起pull request了。 打开B的github,也就是https://github.com/B/durit 点击Pull Requests 单击new pull request 单击create pull request 输入title和你更改的内容 然后单击send pull request 这样B就完成了工作，然后就等着主人A来操作了。 8.在B想要更新A的github上到内容时，结果冲突，因为B和A同时修改了文件，比如说是README.ME，该这样做： 123$ git status(查看冲突文件) //找到冲突文件(README.MD)后，打开并修改，解决冲突后$ git add README.MD$ git commit -m \"解决了冲突文件README.MD\" 现在冲突解决了，可以更新A的内容了，也就是上面第4步和第5步","categories":[{"name":"上古神器","slug":"上古神器","permalink":"https://jokinglove.com/blog/group/categories/上古神器/"}],"tags":[{"name":"git","slug":"git","permalink":"https://jokinglove.com/blog/group/tags/git/"}]},{"title":"hello","slug":"others/hello","date":"2017-04-28T14:00:58.000Z","updated":"2020-10-17T08:07:03.865Z","comments":true,"path":"2017/04/28/others/hello/","link":"","permalink":"https://jokinglove.com/blog/2017/04/28/others/hello/","excerpt":"小计一笔 作为一个做技术的，对所学的技术总结，分享是很重要的，也希望以后多多总结学到的新的东西，分享出来，做一个记录。和更多的人一起去交流，学习。也许，这样才是学习的真正乐趣。","text":"小计一笔 作为一个做技术的，对所学的技术总结，分享是很重要的，也希望以后多多总结学到的新的东西，分享出来，做一个记录。和更多的人一起去交流，学习。也许，这样才是学习的真正乐趣。","categories":[{"name":"瞎写","slug":"瞎写","permalink":"https://jokinglove.com/blog/group/categories/瞎写/"}],"tags":[{"name":"随记","slug":"随记","permalink":"https://jokinglove.com/blog/group/tags/随记/"}]},{"title":"设计模式的六大原则","slug":"others/java/java设计模式的原则","date":"2016-04-28T14:00:58.000Z","updated":"2020-10-17T08:07:03.866Z","comments":true,"path":"2016/04/28/others/java/java设计模式的原则/","link":"","permalink":"https://jokinglove.com/blog/2016/04/28/others/java/java设计模式的原则/","excerpt":"二、设计模式的六大原则 1、开闭原则（Open Close Principle） 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。","text":"二、设计模式的六大原则 1、开闭原则（Open Close Principle） 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。 2、里氏代换原则（Liskov Substitution Principle） 里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。—— From Baidu 百科 3、依赖倒转原则（Dependence Inversion Principle） 这个是开闭原则的基础，具体内容：真对接口编程，依赖于抽象而不依赖于具体。 4、接口隔离原则（Interface Segregation Principle） 这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。还是一个降低类之间的耦合度的意思，从这儿我们看出，其实设计模式就是一个软件的设计思想，从大型软件架构出发，为了升级和维护方便。所以上文中多次出现：降低依赖，降低耦合。 5、迪米特法则（最少知道原则）（Demeter Principle） 为什么叫最少知道原则，就是说：一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。 6、合成复用原则（Composite Reuse Principle） 原则是尽量使用合成/聚合的方式，而不是使用继承。","categories":[{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://jokinglove.com/blog/group/tags/设计模式/"}]},{"title":"Hello World","slug":"others/hello-world","date":"2016-02-14T16:00:00.000Z","updated":"2020-10-17T08:07:03.865Z","comments":true,"path":"2016/02/15/others/hello-world/","link":"","permalink":"https://jokinglove.com/blog/2016/02/15/others/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[{"name":"其他","slug":"其他","permalink":"https://jokinglove.com/blog/group/categories/其他/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://jokinglove.com/blog/group/tags/hexo/"}]}],"categories":[{"name":"database","slug":"database","permalink":"https://jokinglove.com/blog/group/categories/database/"},{"name":"Java","slug":"Java","permalink":"https://jokinglove.com/blog/group/categories/Java/"},{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/categories/IT/"},{"name":"linux","slug":"linux","permalink":"https://jokinglove.com/blog/group/categories/linux/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/categories/kubernetes/"},{"name":"上古神器","slug":"上古神器","permalink":"https://jokinglove.com/blog/group/categories/上古神器/"},{"name":"瞎写","slug":"瞎写","permalink":"https://jokinglove.com/blog/group/categories/瞎写/"},{"name":"其他","slug":"其他","permalink":"https://jokinglove.com/blog/group/categories/其他/"}],"tags":[{"name":"database","slug":"database","permalink":"https://jokinglove.com/blog/group/tags/database/"},{"name":"mongo","slug":"mongo","permalink":"https://jokinglove.com/blog/group/tags/mongo/"},{"name":"springboot","slug":"springboot","permalink":"https://jokinglove.com/blog/group/tags/springboot/"},{"name":"并发","slug":"并发","permalink":"https://jokinglove.com/blog/group/tags/并发/"},{"name":"IT","slug":"IT","permalink":"https://jokinglove.com/blog/group/tags/IT/"},{"name":"java","slug":"java","permalink":"https://jokinglove.com/blog/group/tags/java/"},{"name":"多线程","slug":"多线程","permalink":"https://jokinglove.com/blog/group/tags/多线程/"},{"name":"Database","slug":"Database","permalink":"https://jokinglove.com/blog/group/tags/Database/"},{"name":"tools","slug":"tools","permalink":"https://jokinglove.com/blog/group/tags/tools/"},{"name":"大数据","slug":"大数据","permalink":"https://jokinglove.com/blog/group/tags/大数据/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://jokinglove.com/blog/group/tags/zookeeper/"},{"name":"SPI","slug":"SPI","permalink":"https://jokinglove.com/blog/group/tags/SPI/"},{"name":"webservice","slug":"webservice","permalink":"https://jokinglove.com/blog/group/tags/webservice/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://jokinglove.com/blog/group/tags/kubernetes/"},{"name":"集群","slug":"集群","permalink":"https://jokinglove.com/blog/group/tags/集群/"},{"name":"linux","slug":"linux","permalink":"https://jokinglove.com/blog/group/tags/linux/"},{"name":"Kafka","slug":"Kafka","permalink":"https://jokinglove.com/blog/group/tags/Kafka/"},{"name":"Mongo","slug":"Mongo","permalink":"https://jokinglove.com/blog/group/tags/Mongo/"},{"name":"ES","slug":"ES","permalink":"https://jokinglove.com/blog/group/tags/ES/"},{"name":"搜索","slug":"搜索","permalink":"https://jokinglove.com/blog/group/tags/搜索/"},{"name":"监控","slug":"监控","permalink":"https://jokinglove.com/blog/group/tags/监控/"},{"name":"microservice","slug":"microservice","permalink":"https://jokinglove.com/blog/group/tags/microservice/"},{"name":"cat","slug":"cat","permalink":"https://jokinglove.com/blog/group/tags/cat/"},{"name":"promethus","slug":"promethus","permalink":"https://jokinglove.com/blog/group/tags/promethus/"},{"name":"spring","slug":"spring","permalink":"https://jokinglove.com/blog/group/tags/spring/"},{"name":"python","slug":"python","permalink":"https://jokinglove.com/blog/group/tags/python/"},{"name":"maven","slug":"maven","permalink":"https://jokinglove.com/blog/group/tags/maven/"},{"name":"git","slug":"git","permalink":"https://jokinglove.com/blog/group/tags/git/"},{"name":"1024","slug":"1024","permalink":"https://jokinglove.com/blog/group/tags/1024/"},{"name":"spring-boot","slug":"spring-boot","permalink":"https://jokinglove.com/blog/group/tags/spring-boot/"},{"name":"编码规范","slug":"编码规范","permalink":"https://jokinglove.com/blog/group/tags/编码规范/"},{"name":"面向对象","slug":"面向对象","permalink":"https://jokinglove.com/blog/group/tags/面向对象/"},{"name":"随记","slug":"随记","permalink":"https://jokinglove.com/blog/group/tags/随记/"},{"name":"设计模式","slug":"设计模式","permalink":"https://jokinglove.com/blog/group/tags/设计模式/"},{"name":"hexo","slug":"hexo","permalink":"https://jokinglove.com/blog/group/tags/hexo/"}]}